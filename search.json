[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Bell"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bell’s CSCI 0451 Blog",
    "section": "",
    "text": "Housing Price Prediction\n\n\n\n\n\nA valuable machine learning experience to analyze and predict housing prices.\n\n\n\n\n\n\nMay 19, 2023\n\n\nJiayi Chen & Bell Luo\n\n\n\n\n\n\n  \n\n\n\n\nOptimization with Adam\n\n\n\n\n\nImplementing the Adam optimizer and perform some simple experiments.\n\n\n\n\n\n\nApr 19, 2023\n\n\nBell\n\n\n\n\n\n\n  \n\n\n\n\nTalk by Dr. Gebru\n\n\n\n\n\nGuest Lecture: Eugenics and the Promise of Utopia through Artificial General Intelligence.\n\n\n\n\n\n\nApr 15, 2023\n\n\nBell\n\n\n\n\n\n\n  \n\n\n\n\nUnsupervised Learning\n\n\n\n\n\nSVD as a machine learning approach for image compression.\n\n\n\n\n\n\nApr 12, 2023\n\n\nBell\n\n\n\n\n\n\n  \n\n\n\n\nAuditing Allocative Bias\n\n\n\n\n\nPerform a fairness audit to assess bias with respect demographic characteristics like race or sex.\n\n\n\n\n\n\nMar 29, 2023\n\n\nBell\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\nImplementing least-squares linear regression, and experimenting with LASSO regularization for overparameterized problems.\n\n\n\n\n\n\nMar 15, 2023\n\n\nBell\n\n\n\n\n\n\n  \n\n\n\n\nPenguins\n\n\n\n\n\nDetermining the smallest number of measurements necessary to confidently determine the species of a penguin.\n\n\n\n\n\n\nMar 8, 2023\n\n\nBell\n\n\n\n\n\n\n  \n\n\n\n\nOptimization for Logistic Regression\n\n\n\n\n\nImplementing simple gradient descent, a momentum method, and stochastic gradient descent; comparing their performance for training logistic regression.\n\n\n\n\n\n\nMar 1, 2023\n\n\nBell\n\n\n\n\n\n\n  \n\n\n\n\nPerceptron\n\n\n\n\n\nImplementing the perceptron algorithm using numerical programming, and demonstrating its use on synthetic data sets.\n\n\n\n\n\n\nFeb 26, 2023\n\n\nBell\n\n\n\n\n\n\n  \n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/adam/index.html",
    "href": "posts/adam/index.html",
    "title": "Optimization with Adam",
    "section": "",
    "text": "Image credit: Artificial Neural Networks: Some Misconceptions (Part 3)\nPython source: 451-blog/adam.py at main · doabell/451-blog\nInstructions can be found at Optimization with Adam."
  },
  {
    "objectID": "posts/adam/index.html#introduction",
    "href": "posts/adam/index.html#introduction",
    "title": "Optimization with Adam",
    "section": "Introduction",
    "text": "Introduction\nKingma & Ba 2017 introduces Adam, an algorithm for gradient optimization that is:\n\nstraightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters.\n\nIn a previous blog post, we implemented logistic regression with:\n\ngradient descent,\nstochastic gradient descent, and\nstochastic gradient descent with the momentum method.\n\nTherefore, in this blog post, we will implement the Adam algorithm for Logistic Regression, and compare with these previous methods."
  },
  {
    "objectID": "posts/adam/index.html#implementation",
    "href": "posts/adam/index.html#implementation",
    "title": "Optimization with Adam",
    "section": "Implementation",
    "text": "Implementation\nHere is Page 2 of the original paper, with line numbers that are referenced in the source file:\n\n\n\nAlgorithm 1 from Adam\n\n\nThis implementation calculates logistic loss with the method outlined in this blog post. More details discussed below."
  },
  {
    "objectID": "posts/adam/index.html#basic-experiments",
    "href": "posts/adam/index.html#basic-experiments",
    "title": "Optimization with Adam",
    "section": "Basic experiments",
    "text": "Basic experiments\nWe compare the performance of Adam with previous algorithms implemented in Optimization for Logistic Regression.\nHere is a generated dataset:\n\n# Data\nimport numpy as np\nfrom adam import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nfrom matplotlib import font_manager\nfont_manager.fontManager.addfont(\"C:\\Windows\\Fonts\\FiraSans-Regular.ttf\")\nplt.rcParams[\"font.family\"] = \"Fira Sans\"\nnp.seterr(all='ignore')\nnp.random.seed(283)\n\n# make the data\np_features = 3\nX, y = make_blobs(\n    n_samples=500, n_features=p_features - 1, centers=[(-1, -1), (1, 1)]\n)\n\nfig = plt.scatter(X[:, 0], X[:, 1], c=y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nTime\nWe first examine the runtime and number of epochs for optimization with Adam.\nVisualization code adapted from blog post instructions.\n\nmax_epochs = 100000\nalpha = 0.001\n\nLR_base = LogisticRegression()\nLR_sg = LogisticRegression()\nLR_sgm = LogisticRegression()\nLR_adam = LogisticRegression()\n\n\n%%timeit\nLR_base.fit(X, y, alpha=alpha, max_epochs=max_epochs)\n\n2.07 s ± 250 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n%%timeit\nLR_sg.fit_stochastic(\n    X, y,\n    max_epochs=max_epochs,\n    momentum=False,\n    batch_size=10,\n    alpha=alpha\n)\n\n1.82 s ± 93.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n%%timeit\nLR_sgm.fit_stochastic(\n    X, y,\n    max_epochs=max_epochs,\n    momentum=True,\n    batch_size=10,\n    alpha=alpha\n)\n\n1.81 s ± 69.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n%%timeit\nLR_adam.fit_adam(\n    X, y,\n    alpha=alpha,\n    max_epochs=max_epochs\n)\n\n639 ms ± 76.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\nplt.plot(\n    np.arange(len(LR_base.loss_history)) + 1,\n    LR_base.loss_history,\n    label=f\"Gradient descent, {len(LR_base.loss_history)} epochs\"\n)\n\nplt.plot(\n    np.arange(len(LR_sg.loss_history)) + 1,\n    LR_sg.loss_history,\n    label=f\"Stochastic, {len(LR_sg.loss_history)} epochs\"\n)\n\nplt.plot(\n    np.arange(len(LR_sgm.loss_history)) + 1,\n    LR_sgm.loss_history,\n    label=f\"Momentum, {len(LR_sgm.loss_history)} epochs\"\n)\n\nplt.plot(\n    np.arange(len(LR_adam.loss_history)) + 1,\n    LR_adam.loss_history,\n    label=f\"Adam, {len(LR_adam.loss_history)} epochs\"\n)\n\nplt.loglog()\n\nlegend = plt.legend(loc=\"upper right\")\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\n\nEpochs\nWe observe that Adam is the fastest, converging in only 241 epochs.\nThis is much faster than stochastic gradient descent, with or without the momentum method. Both of these converged in around 1000 epochs.\nNormal gradient descent is the slowest, failing to converge after 10000 epochs.\n\n\nWall time\nUsing the %%timeit magic command from IPython:\n\nAdam has the fastest runtime of 639 ms.\nStocastic gradient descent, with or without momentum, took around 1.82 s.\n“Plain” gradient descent took 2.07 s, although 2.07 s is exactly 1 standard deviation away from 1.82 s.\n\n\n\n\nLearning rate\nThe learning rate determines how much we move along the gradient.\nWhen the learning rate is too large, gradient descent will exceed the optimal minimum; when it is too small, gradient descent will take longer to reach the optimal minimum.\nWe loop our Adam implementation through multiple learning rates:\n\nalphas = [0.5, 0.05, 0.0005]\nmax_epochs = 1000\n\nfor alpha in alphas:\n    LR = LogisticRegression()\n    LR.fit_adam(\n        X, y,\n        max_epochs=max_epochs,\n        batch_size=10,\n        alpha=alpha\n    )\n\n    num_steps = len(LR.loss_history)\n    plt.plot(\n        np.arange(num_steps) + 1, LR.loss_history,\n        label=f\"alpha = {alpha}, {num_steps} epochs\",\n        alpha=0.6 if alpha == 0.5 else 1\n    )\n\nplt.loglog()\n\nlegend = plt.legend()\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nWith three different learning rates, we can see a difference in how training loss progressed over time:\n\nReasonably small, alpha = 0.05: loss decreased quickly, converging in only 11 iterations.\nToo small, alpha = 0.0005: loss decreased more smoothly but slowly, converging in 409 iterations.\nToo large, alpha = 0.5: loss jumped up and down frequently, converging in 743 iterations.\n\nThis shows that indeed, we want a reasonably small learning rate. Small enough to converge, but not too small that converging becomes too slow.\n\n\nBatch size\nWe look at another hyperparameter, the batch size.\nBecause we look at each “batch” separately, the size of these batches, might determine the speed of gradient descent.\nUsing our reasonable learning rate of alpha = 0.05:\n\nfrom time import perf_counter\nbatch_sizes = [2, 10, 20, 50, 100, 200, 500]\nrepeat = 5\n\nfor batch_size in batch_sizes:\n    times = []\n    for _ in range(repeat):\n        LR = LogisticRegression()\n        start = perf_counter() * 1000\n        LR.fit_adam(\n            X, y,\n            max_epochs=1000,\n            batch_size=batch_size,\n            alpha=0.05\n        )\n        end = perf_counter() * 1000\n        times.append(end - start)\n\n    perf = sum(times) / len(times)\n\n    num_steps = len(LR.loss_history)\n    plt.plot(\n        np.arange(num_steps) + 1, LR.loss_history,\n        label=f\"batch size = {batch_size:4}, {perf:&gt; 6.2f}ms, {num_steps} epochs\",\n        alpha=0.7\n    )\n\nplt.loglog()\n\nlegend = plt.legend()\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nHere, using otherwise identical hyperparameters, we observe that batch size does affect convergence speed and runtime.\nA batch size of 2 yielded the most epochs and the longest runtime.\nA batch size of 50 converged in the least epochs, although it is slightly slower than a batch size of 100 and 200, perhaps due to more calculations per loop.\nLarger batch sizes took more epochs to converge, while maintaining around the same wall clock time.\nIn conclusion, a reasonable batch size (in this case, 100) strikes a nice balance between wall clock time and the number of epochs needed to converge."
  },
  {
    "objectID": "posts/adam/index.html#digits",
    "href": "posts/adam/index.html#digits",
    "title": "Optimization with Adam",
    "section": "Digits",
    "text": "Digits\nWe load the digits dataset included with scikit-learn.\nBecause we are predicting binary lables, we choose only the numbers 4 and 9. Then, we normalize the X and y labels. Finally, we perform a train-test split.\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\n\ndigits = load_digits()\nX = digits.data\ny = digits.target\nX = X[np.logical_or(y == 4, y == 9)]\ny = y[np.logical_or(y == 4, y == 9)]\n\nX = normalize(X, axis=0)\ny = np.where(y == 4, 0, 1)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=283\n)\n\n\nmax_epochs = 1000\nalpha = 2\nbatch_size = 5\n\nLR_sgm = LogisticRegression()\nLR_adam = LogisticRegression()\n\n\nPerformance\nHere, we only execute %%timeit once, because we will actually need the regression results later.\n\n%%timeit -n1 -r1\nLR_sgm.fit_stochastic(\n    X_train, y_train,\n    max_epochs=max_epochs,\n    momentum=False,\n    alpha=alpha,\n    batch_size=batch_size\n)\n\n2.37 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n%%timeit -n1 -r1\nLR_adam.fit_adam(\n    X_train, y_train,\n    alpha=alpha,\n    max_epochs=max_epochs,\n    batch_size=batch_size\n)\n\n502 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\nplt.plot(\n    np.arange(len(LR_sgm.loss_history)) + 1,\n    LR_sgm.loss_history,\n    label=f\"Stochastic momentum, {len(LR_sgm.loss_history)} epochs\"\n)\n\nplt.plot(\n    np.arange(len(LR_adam.loss_history)) + 1,\n    LR_adam.loss_history,\n    label=f\"Adam, {len(LR_adam.loss_history)} epochs\"\n)\n\nplt.loglog()\n\nlegend = plt.legend()\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nWith the same learning rate and batch size, stochastic-momentum did not converge in 1000 epochs.\nOn the other hand, Adam converged much faster than stochastic-momentum, and to a lower loss.\n\n\nUnderflow\nIn our original implementation, Adam gave nan losses.\nSince the losses dropped below 10^{-5}, there was apparently some underflow.\nOne workaround is to treat nan loss as small enough, but this will only work with underflow and not other situations.\nAnother workaround is discussed in the blog post How to Evaluate the Logistic Loss and not NaN trying by Fabian Pedregosa and Bart van Merriënboer, which builds upon ideas from Martin Mächler.\nIn short, the new log-sig function now looks like this:\n\n\\log(s(t)) = \\begin{cases}\n    t & t &lt; -33.3\\\\\n    t - \\exp(t) & -33.3 \\leq t \\leq -18 \\\\\n    -\\text{log1p}(\\exp(-t)) & -18 \\leq t \\leq 37 \\\\\n    -\\exp(-t) & 37 \\leq t\\\\\n\\end{cases}\n\nThe blog post continues to rewrite the gradient calculation step, which is not needed here at this point.\n\n\nAccuracy\nWe now test the accuracy of the two fitted models:\n\nsgm_score = LR_sgm.score(X_test, y_test)\nadam_score = LR_adam.score(X_test, y_test)\n\nprint(f\"Momentum: {sgm_score:.4f}\")\nprint(f\"Adam    : {adam_score:.4f}\")\n\nfrom sklearn.metrics import confusion_matrix\n\npred = LR_adam.predict(X_test)\n\n\nprint(\"\\nConfusion matrix for Adam: \")\nprint(\"   4  9\")\nprint(confusion_matrix(y_test, pred))\n\nMomentum: 0.9890\nAdam    : 0.9890\n\nConfusion matrix for Adam: \n   4  9\n[[46  1]\n [ 0 44]]\n\n\nAdam achieved an accuracy of 98.63%, with only 1 mis-classified number.\nThe momentum method, had the same accuracy, so also 1 mis-classified number.\n\n_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))\nfor ax, image, label, prediction in zip(axes, X_test[87:], y_test[87:],  pred[87:]):\n    ax.set_axis_off()\n    image = image.reshape(8, 8)\n    ax.imshow(image, cmap=plt.cm.gray_r, interpolation=\"nearest\")\n    ax.set_title(f\"Label: {9 if label else 4}\\nPrediction: {9 if prediction else 4}\")\n\n\n\n\nVisually, the misclassification looks ambiguous, to the extent that correctly classifying this might be a sign of overfitting."
  },
  {
    "objectID": "posts/adam/index.html#breast-cancer",
    "href": "posts/adam/index.html#breast-cancer",
    "title": "Optimization with Adam",
    "section": "Breast Cancer",
    "text": "Breast Cancer\nThe next experiment is performed on the Breast Cancer Wisconsin (Diagnostic) Data Set, which includes 569 samples and 30 dimensions.\nAlthough this dataset has been extensively studied, it still carries more ethical implications than, say, handwritten digits.\nFor the targets, 1 gives a benign (noncancerous) diagnosis, and 0 gives a malignant (cancerous) diagnosis.\n\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX = cancer.data\ny = cancer.target\n\nX = normalize(X, axis=0)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n\n\nmax_epochs = 100000\nalpha = 0.05\nbatch_size = 10\n\nLR_sgm = LogisticRegression()\nLR_adam = LogisticRegression()\n\nWe repeat the experiments in the Digits section.\n\nPerformance\n\n%%timeit -n1 -r1\nLR_sgm.fit_stochastic(\n    X_train, y_train,\n    max_epochs=max_epochs,\n    momentum=False,\n    alpha=alpha,\n    batch_size=batch_size\n)\n\n33.5 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n%%timeit -n1 -r1\nLR_adam.fit_adam(\n    X_train, y_train,\n    alpha=alpha,\n    max_epochs=max_epochs,\n    batch_size=batch_size\n)\n\n1.27 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\nplt.plot(\n    np.arange(len(LR_sgm.loss_history)) + 1,\n    LR_sgm.loss_history,\n    label=f\"Stochastic momentum, {len(LR_sgm.loss_history)} epochs\"\n)\n\nplt.plot(\n    np.arange(len(LR_adam.loss_history)) + 1,\n    LR_adam.loss_history,\n    label=f\"Adam, {len(LR_adam.loss_history)} epochs\"\n)\n\nplt.loglog()\n\nlegend = plt.legend(loc=\"upper right\")\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nWith the same learning rate and batch size, Adam converged much faster than stochastic-momentum, both in terms of epochs and wall clock time.\n\n\nAccuracy\nWe now test the accuracy of the two fitted models:\n\nsgm_score = LR_sgm.score(X_test, y_test)\nadam_score = LR_adam.score(X_test, y_test)\n\nprint(f\"Momentum: {sgm_score:.4f}\")\nprint(f\"Adam    : {adam_score:.4f}\")\n\nfrom sklearn.metrics import confusion_matrix\n\npred = LR_sgm.predict(X_test)\n\n\nprint(\"\\nConfusion matrix for Momentum: \")\nprint(\"   0  1\")\nprint(confusion_matrix(y_test, pred))\n\npred = LR_adam.predict(X_test)\n\n\nprint(\"\\nConfusion matrix for Adam: \")\nprint(\"   0  1\")\nprint(confusion_matrix(y_test, pred))\n\nMomentum: 0.9790\nAdam    : 0.9860\n\nConfusion matrix for Momentum: \n   0  1\n[[52  2]\n [ 1 88]]\n\nConfusion matrix for Adam: \n   0  1\n[[53  1]\n [ 1 88]]\n\n\nWe can see that Adam performed slightly better than the momentum method, mis-classifying one less cancerous diagnosis as noncancerous.\nThis variation, however, might be the result of randomness instead of an actual accuracy difference."
  },
  {
    "objectID": "posts/adam/index.html#conclusion",
    "href": "posts/adam/index.html#conclusion",
    "title": "Optimization with Adam",
    "section": "Conclusion",
    "text": "Conclusion\nAs we have seen, Adam is an efficient optimization on stochastic gradient descent, cutting both the number of epochs and the wall clock time.\nBecause these are initialized as zeroes, the authors corrects that bias by introducing \\hat{m_t} and \\hat{v_t}.\nThrough interacting with a visualization, one can observe that Adam can achieve the global minima faster and more often than SGD, momentum, and RMSProp.\nIn implementing Adam, we also encountered underflow, which is an important aspect of applying mathematical models to a computer program. It also highlights the importance of using existing frameworks, since these are often well-tested.\nAdam itself is built upon AdaGrad and RMSProp. This highlights that work in the field is often built upon others’ work.\n\nEffectiveness\nThe name “Adam” comes from “adaptive moment estimation”, referring to both the mean m_t (Line 7, “first moment”) and variance v_t (Line 8, “second moment”).\nAs explained in Section 2, Adam’s effectiveness can be partly explained by two aspects:\n\nThe learning rate decays exponentially, as suggested by \\alpha_t = \\alpha \\cdot \\sqrt{1-\\beta_2^t} / (1-\\beta_1^t). This establishes a “trust region” so that \\alpha has the right scale.\nThe ratio of m_t \\cdot \\sqrt{v_t} is a “signal-to-noise ratio”, determining how much to move the weights vector. If the SNR is small, there is greater uncertainty in the weights, so the algorithm is more cautious.\n\nThese are combined in the more efficient algorithm proposed in section 2, which is essentially moving \\theta by the product of \\alpha_t and m_t \\cdot \\sqrt{v_t}."
  },
  {
    "objectID": "posts/auditing-bias/index.html",
    "href": "posts/auditing-bias/index.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "from matplotlib import font_manager\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom os.path import isfile\nimport warnings\nwarnings.filterwarnings('ignore')\nfont_manager.fontManager.addfont(\"C:\\Windows\\Fonts\\FiraSans-Regular.ttf\")\n\nsns.set_theme()\nsns.set(font=\"Fira Sans\")\nImage credits: Christina Animashaun/Vox\nInstructions can be found at Auditing Allocative Bias."
  },
  {
    "objectID": "posts/auditing-bias/index.html#problem-statement",
    "href": "posts/auditing-bias/index.html#problem-statement",
    "title": "Auditing Allocative Bias",
    "section": "Problem statement",
    "text": "Problem statement\nWe grab datasets derived from the US Census, courtesy of the folktables package.\nWe can use BasicProblem() to frame a problem for our investigation.\nIn this case, we attempt to predict employment status on the basis of demographics, excluding race, in the US state of California.\nSince we are looking at employment, it would make sense to only look at people between the ages of 16 and 90.\nThen, we audit for racial bias in our predictions.\n(The code chunk below is derived from the blog post instructions.)\n\n# Data\nfrom folktables import ACSDataSource, BasicProblem\n\nSTATE = \"CA\"\n\n\ndef age_filter(data):\n    \"\"\"Age filter.\n    \"\"\"\n    df = data\n    df = df[df['AGEP'] &gt; 16]\n    df = df[df['AGEP'] &lt; 90]\n    return df\n\n\ndata_source = ACSDataSource(\n    survey_year='2018',\n    horizon='1-Year',\n    survey='person'\n)\n\nfile_exists = isfile(\"./data/2018/1-Year/psam_p06.csv\")\nacs_data = data_source.get_data(states=[STATE], download=not file_exists)\n\npossible_features = [\n    'AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG',\n    'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR'\n]\n\nfeatures_to_use = [\n    f for f in possible_features\n    if f not in [\"ESR\", \"RAC1P\"]\n]\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=age_filter,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nThen, we perform a train-test split:\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, \\\n    y_train, y_test, \\\n    group_train, group_test = train_test_split(\n        features,\n        label,\n        group,\n        test_size=0.2,\n        random_state=0\n    )"
  },
  {
    "objectID": "posts/auditing-bias/index.html#data-descriptives",
    "href": "posts/auditing-bias/index.html#data-descriptives",
    "title": "Auditing Allocative Bias",
    "section": "Data descriptives",
    "text": "Data descriptives\nWe answer some descriptive questions about the data. In order to do this, we convert our training set back into a dataframe:\n\ndf = pd.DataFrame(X_train, columns=features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\ndf.head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\ngroup\nlabel\n\n\n\n\n0\n39.0\n21.0\n3.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nTrue\n\n\n1\n63.0\n22.0\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1\nFalse\n\n\n2\n36.0\n21.0\n1.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n4.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nTrue\n\n\n3\n17.0\n14.0\n5.0\n2.0\n2.0\n1.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n1.0\n9\nFalse\n\n\n4\n78.0\n21.0\n3.0\n0.0\n2.0\n0.0\n4.0\n1.0\n4.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2.0\n6\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn definitions (click to expand)\n\n\n\n\n\nThese are the column definitions from Appendix B.2 of the original paper:\n\nAGEP: age\nSCHL: education, from 1 (none) to 24 (doctorate degree)\nMAR, RELP: marital & relationship status\nDIS, DEAR, DEYE, DREM: disabilities\nESP: employment status of parents\nCIT: citizenship\nMIG: mobility status\nMIL: military service\nANC: ancestry\nSEX: 1 for male, 2 for female\nlabel: employed (True) or unemployed (False)\n\nThe group column encodes race as follows:\n\nWhite alone\nBlack or African American alone\nAmerican Indian alone\nAlaska Native alone\nAmerican Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races\nAsian alone\nNative Hawaiian and Other Pacific Islander alone\nSome Other Race alone\nTwo or More Races\n\n\n\n\nWe can use pandas.DataFrame.describe to get some basic descriptive statistics:\n\ndf[[df.columns[0]]].describe()\n\n\n\n\n\n\n\n\nAGEP\n\n\n\n\ncount\n242112.000000\n\n\nmean\n47.426612\n\n\nstd\n18.480978\n\n\nmin\n17.000000\n\n\n25%\n32.000000\n\n\n50%\n47.000000\n\n\n75%\n62.000000\n\n\nmax\n89.000000\n\n\n\n\n\n\n\nSince AGEP (age) is quantitiative, describe gives us the age percentiles.\nWe can see 242,112 people in the dataset, with age ranging from 17 to 89 (as filtered), and averaging at 47.42.\nThen, we look at the other columns, which are qualitative:\n\n# https://stackoverflow.com/a/37717675\nsummary = df.drop(columns=\"AGEP\", axis=1).astype(int).astype(str).describe()\n\n# percentage of mode\nsummary.loc[\"percent\"] = (summary.loc[\"freq\"] /\n                          summary.loc[\"count\"] * 100).astype(float).round(2)\nsummary\n\n# I miss R, they have f-strings but for markdown\n# I realize I included children, but too late\n# so now I have to manually edit every single number\n# https://quarto.org/docs/computations/execution-options.html#inline-code\n\n\n\n\n\n\n\n\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\ngroup\nlabel\n\n\n\n\ncount\n242112\n242112\n242112\n242112\n242112\n242112\n242112\n242112\n242112\n242112\n242112\n242112\n242112\n242112\n242112\n242112\n\n\nunique\n24\n5\n18\n2\n9\n5\n3\n4\n4\n2\n2\n2\n2\n2\n9\n2\n\n\ntop\n21\n1\n0\n2\n0\n1\n1\n4\n1\n1\n2\n2\n2\n2\n1\n1\n\n\nfreq\n49765\n123618\n106112\n209200\n238702\n163044\n210673\n225554\n151602\n166490\n233094\n236350\n229852\n123410\n151265\n138066\n\n\npercent\n20.55\n51.06\n43.83\n86.41\n98.59\n67.34\n87.01\n93.16\n62.62\n68.77\n96.28\n97.62\n94.94\n50.97\n62.48\n57.03\n\n\n\n\n\n\n\nThis table gives us insight into the different qualitative categories.\nFor example:\n\nSCHL: the most common level of education in our dataset is 21 (“Bachelor’s degree”) at 49,765 people (20.55%).\nCIT: the majority of people (163,044, 67.34 %) in our dataset were born in the U.S.\ngroup: 62.48% of our dataset (151,265 people) are White.\nlabel: 57.03% of people in our dataset are unemployed. Therefore, 42.97% of people in our dataset are employed.\n\nWe can then look at each group with pandas.DataFrame.value_counts:\n\n# this took so much googling\n# now I really miss R\n# https://stackoverflow.com/a/69573365\nfreq = df[[\"group\", \"label\"]].value_counts().unstack(fill_value=0)\nfreq[\"Total\"] = freq[False] + freq[True]\nperc = df[[\"group\", \"label\"]].value_counts(\n    normalize=True).unstack(fill_value=0)\n# Total and in_group percentages\nperc[\"Total\"] = (freq[False] + freq[True]) / df.shape[0]\nperc[\"True_in_group\"] = freq[True] / (freq[False] + freq[True])\nperc[\"False_in_group\"] = 1 - perc[\"True_in_group\"]\n\n# https://stackoverflow.com/a/53679333\npd.concat([freq, perc * 100], axis=1, keys=['counts', '%']).round(3)\n\n\n\n\n\n\n\n\ncounts\n%\n\n\nlabel\nFalse\nTrue\nTotal\nFalse\nTrue\nTotal\nTrue_in_group\nFalse_in_group\n\n\ngroup\n\n\n\n\n\n\n\n\n\n\n\n\n1\n66113\n85152\n151265\n27.307\n35.170\n62.477\n56.293\n43.707\n\n\n2\n6303\n5749\n12052\n2.603\n2.375\n4.978\n47.702\n52.298\n\n\n3\n929\n884\n1813\n0.384\n0.365\n0.749\n48.759\n51.241\n\n\n4\n12\n7\n19\n0.005\n0.003\n0.008\n36.842\n63.158\n\n\n5\n244\n304\n548\n0.101\n0.126\n0.226\n55.474\n44.526\n\n\n6\n15837\n23588\n39425\n6.541\n9.743\n16.284\n59.830\n40.170\n\n\n7\n332\n454\n786\n0.137\n0.188\n0.325\n57.761\n42.239\n\n\n8\n10551\n16315\n26866\n4.358\n6.739\n11.097\n60.727\n39.273\n\n\n9\n3725\n5613\n9338\n1.539\n2.318\n3.857\n60.109\n39.891\n\n\n\n\n\n\n\nThe interpretation of this table is fairly straighforward.\nFor example, of our dataset, 62.477% (151,265 people) is from group 1 (White alone), 35.170% (85,152 people) are employed, and 27.307% (66,113 people) are unemployed.\nInside group 1, 56.293% are employed, so 43.707% are not.\n\nIntersectional trends\n\nSex\nWe now look at employment data that is broken out by our group variable (race) and other variables.\nFirst, employment by race and sex:\n\np = sns.catplot(data=df, x=\"group\", y=\"label\", kind=\"bar\", errorbar=None)\naxes = p.axes.flatten()\naxes[0].set_ylabel(\"Employed\")\naxes[0].set_xlabel(\"Race group\")\n\np = sns.catplot(data=df, x=\"SEX\", y=\"label\", kind=\"bar\", errorbar=None)\naxes = p.axes.flatten()\naxes[0].set_ylabel(\"Employed\")\naxes[0].set_xlabel(\"Sex\")\n\nText(0.5, 12.444444444444416, 'Sex')\n\n\n\n\n\n\n\n\nWe observe stark differences between the different race and sex groups.\nThe employment rate for race group 4 (Alaska Native alone) is particularly low; this may or may not be explained by the small sample size of 19 people.\n\np = sns.catplot(data=df, x=\"group\", y=\"label\", hue=\"SEX\", kind=\"bar\", errorbar=None)\naxes = p.axes.flatten()\naxes[0].set_ylabel(\"Employed\")\naxes[0].set_xlabel(\"Race group\")\n\nText(0.5, 34.062499999999986, 'Race group')\n\n\n\n\n\nPutting race and sex together, we still observe a clear difference between employment rates across groups.\nAs for sex across race groups, only groups 2 (Black or African American alone) and 5 (American Indian and Alaska Native tribes specified) have more female employment than men. In other race groups, men are more employed.\n\n\nMilitary service\n\np = sns.catplot(data=df, x=\"group\", y=\"label\", hue=\"MIL\", kind=\"bar\", errorbar=None)\naxes = p.axes.flatten()\naxes[0].set_ylabel(\"Employed\")\naxes[0].set_xlabel(\"Race group\")\n\nText(0.5, 34.062499999999986, 'Race group')\n\n\n\n\n\nWhen grouped by military service, we see that:"
  },
  {
    "objectID": "posts/auditing-bias/index.html#model-training",
    "href": "posts/auditing-bias/index.html#model-training",
    "title": "Auditing Allocative Bias",
    "section": "Model training",
    "text": "Model training\nWe will be using sklearn.ensemble.RandomForestClassifier, which trains forests of decision trees to predict employment based on the predictors mentioned above.\nThe hyperparameter in random forest that we will select is max_depth, how deep the tree can go before giving a result. We loop values of max_depth between 3 and 15.\nThere are other hyperparameters and other, more proper selection methods available, but given the size of the California dataset, we will be using a for loop only.\nWe proceed to scale our variables first with a pipeline:\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\n\nscores = {}\n\nrepeat = 3\n\nfor d in range(5, 25):\n\n    score = []\n\n    for i in range(repeat):\n        X_tune, X_val, \\\n        y_tune, y_val = train_test_split(\n            X_train,\n            y_train,\n            test_size=0.2,\n            random_state=d+i+42\n        )\n        \n        rf = RandomForestClassifier(max_depth=d, random_state=42, n_jobs=-1)\n\n        rfc = Pipeline([\n            (\"scale\", StandardScaler()),\n            (\"rf\", rf)\n        ])\n\n        rfc.fit(X_tune, y_tune)\n        \n        score.append(rfc.score(X_val, y_val))\n\n    scores[d] = sum(score) / len(score)\n\n\np = sns.relplot(x=scores.keys(), y=scores.values(), kind=\"line\")\naxes = p.axes.flatten()\naxes[0].set_xlabel(\"Max depth\")\naxes[0].set_ylabel(\"CV Score\")\n\nText(-11.180555555555555, 0.5, 'CV Score')\n\n\n\n\n\nAt around 0.77, the cross-validation scores do not differ much.\nHere, we choose the best-performing max depth, 15, and fit the actual model:\n\ndepth = 15\n\nrf = RandomForestClassifier(max_depth=depth, random_state=42, n_jobs=-1)\n\nrfc = Pipeline([\n    (\"scale\", StandardScaler()),\n    (\"rf\", rf)\n])\n\nrfc.fit(X_train, y_train)\n\nPipeline(steps=[('scale', StandardScaler()),\n                ('rf',\n                 RandomForestClassifier(max_depth=15, n_jobs=-1,\n                                        random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('scale', StandardScaler()),\n                ('rf',\n                 RandomForestClassifier(max_depth=15, n_jobs=-1,\n                                        random_state=42))])StandardScalerStandardScaler()RandomForestClassifierRandomForestClassifier(max_depth=15, n_jobs=-1, random_state=42)"
  },
  {
    "objectID": "posts/auditing-bias/index.html#auditing",
    "href": "posts/auditing-bias/index.html#auditing",
    "title": "Auditing Allocative Bias",
    "section": "Auditing",
    "text": "Auditing\nNext, we can audit our fitted model for bias.\n\nOverall\n\nfrom sklearn.metrics import confusion_matrix\n\nprint(f\"Accuracy: {rfc.score(X_test, y_test):.4f}\")\n\ny_pred = rfc.predict(X_test)\n\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n\nprint(f\"PPV: {tp / (tp + fp):.4f}\")\nprint(f\"FNR: {fn / (fn + tp):.4f}\")\nprint(f\"FPR: {fp / (fp + tn):.4f}\")\n\nAccuracy: 0.7752\nPPV: 0.7628\nFNR: 0.1230\nFPR: 0.3587\n\n\nOverall, our model has an accuracy of 0.7752.\nThe positive predictive value is 0.7628; the overall false negative and false positive rates are 0.1230 and 0.3587.\n\n\nBy-Group\n\ngroups_score = pd.DataFrame(columns=[\"Group\", \"Accuracy\", \"PPV\", \"TPR\", \"FNR\", \"FPR\"])\n\nfor g in range(1, 10):\n    X_test_g = X_test[group_test == g]\n    y_test_g = y_test[group_test == g]\n\n    y_pred_g = rfc.predict(X_test_g)\n\n    tn, fp, fn, tp = confusion_matrix(y_test_g, y_pred_g).ravel()\n\n    groups_score.loc[g] = [g, rfc.score(X_test_g, y_test_g), tp / (tp + fp), tp / (tp + fn), fn / (fn + tp), fp / (fp + tn)]\n\n\n# https://stackoverflow.com/a/44941463\ng_melt = groups_score.melt(\"Group\")\n# p = sns.catplot(data=g_melt, x=\"variable\", y=\"value\", hue=\"Group\", kind=\"point\", errorbar=None)\np = sns.catplot(data=g_melt, x=\"variable\", y=\"value\", col=\"Group\", kind=\"bar\", col_wrap=3, errorbar=None)\n# https://stackoverflow.com/a/67594395\nfor ax in p.axes.ravel():\n    # add annotations\n    for c in ax.containers:\n        labels = [f\"{v.get_height():.4f}\" for v in c]\n        ax.bar_label(c, labels=labels, label_type='edge')\n    ax.margins(y=0.2)\n\n\n\n\n\n\nBias measures\nWe will considering group 4 as an outlier, since it had only 12 samples.\nThe impact on that group (Alaska Native alone) will be discussed in a later section.\n\nCalibration\nWe compare the true positive rates (green) across different groups.\nThe difference is noticable, especially between group 5 (0.842) and group 6 (0.908), at around 0.06.\nTherefore, we can say this model does not achieve calibration.\n\n\nError rate balance\nThe differences in error rates (FNR in red, FPR in purple) is noticable.\nFor FNR, the greatest difference is between group 3 (0.140) and group 6 (0.092). at around 0.05.\nFor FPR, the greatest difference is between group 2 (0.300) and group 6 (0.422). at around 0.12.\nTherefore, we can say this model does not achieve error rate balance.\n\n\nStatistical parity\nThe PPV rates (orange) across groups above vary from 0.672 (for group 3) to 0.768 (for group 1).\nThis amounts to a staggering difference larger than 0.09 (almost 1 in 10 people).\nTherefore, we can say this model does not satisfy statistical parity.\n\n\n\nIntersectional bias\nWe calculate the FNR for Black women, Black men, White women, and White men.\n\ndf = pd.DataFrame(X_test, columns = features_to_use)\n\nX_tbw = X_test[(group_test == 2) & (df[\"SEX\"] == 2)]\ny_tbw = y_test[(group_test == 2) & (df[\"SEX\"] == 2)]\n\ntn, fp, fn, tp = confusion_matrix(y_tbw, rfc.predict(X_tbw)).ravel()\n\nprint(f\"FNR for Black women: {fn / (fn + tp)}\")\n\nX_tbm = X_test[(group_test == 2) & (df[\"SEX\"] == 1)]\ny_tbm = y_test[(group_test == 2) & (df[\"SEX\"] == 1)]\n\ntn, fp, fn, tp = confusion_matrix(y_tbm, rfc.predict(X_tbm)).ravel()\n\nprint(f\"FNR for Black men:   {fn / (fn + tp)}\")\n\nX_tww = X_test[(group_test == 1) & (df[\"SEX\"] == 2)]\ny_tww = y_test[(group_test == 1) & (df[\"SEX\"] == 2)]\n\ntn, fp, fn, tp = confusion_matrix(y_tww, rfc.predict(X_tww)).ravel()\n\nprint(f\"FNR for White women: {fn / (fn + tp)}\")\n\nX_twm = X_test[(group_test == 1) & (df[\"SEX\"] == 1)]\ny_twm = y_test[(group_test == 1) & (df[\"SEX\"] == 1)]\n\ntn, fp, fn, tp = confusion_matrix(y_twm, rfc.predict(X_twm)).ravel()\n\nprint(f\"FNR for White men:   {fn / (fn + tp)}\")\n\nFNR for Black women: 0.15739484396200815\nFNR for Black men:   0.09672619047619048\nFNR for White women: 0.15678137651821863\nFNR for White men:   0.11417530300368874\n\n\nAlthough the FNR for Black women is around the same for White women, it is still higher than the FNR for White men, which is still higher than for Black men."
  },
  {
    "objectID": "posts/auditing-bias/index.html#conclusion",
    "href": "posts/auditing-bias/index.html#conclusion",
    "title": "Auditing Allocative Bias",
    "section": "Conclusion",
    "text": "Conclusion\n\nWhat groups of people could stand to benefit from a system that is able to predict the label you predicted, such as income or employment status?\nFor example, what kinds of companies might want to buy your model for commercial use?\n\nCompanies who want their customers to be employed, such as insurance companies or banks giving out loans, might want to predict a person’s employment status.\nPeople benefitted will have disproportionate access to such financial resources.\n\nBased on your bias audit, what could be the impact of deploying your model for large-scale prediction in commercial or governmental settings?\n\nThe impact will be inequality in financial tools and resources, which can compound to large differences in wealth, quality of life, and even life expectancy.\n\nBased on your bias audit, do you feel that your model displays problematic bias? What kind (calibration, error rate, etc)?\n\nThe model displays calibrational bias, error rate bias, and intersectional bias, across the 9 race groups surveyed, despite not including race in the fitting process.\n\nBeyond bias, are there other potential problems associated with deploying your model that make you uncomfortable?\nHow would you propose addressing some of these problems?\n\nThat this model is a reflection of the ongoing injustices concerning the group surveyed, since the model fitting did not take race into account.\n\nSome of the variables surveyed might also not be changable, in the short run or in a lifetime, so perhaps these variables should be stripped from our model, or at least be subject to scrutiny like that in, say, the Fair Housing Act."
  },
  {
    "objectID": "posts/dr-gebru-talk/index.html",
    "href": "posts/dr-gebru-talk/index.html",
    "title": "Talk by Dr. Gebru",
    "section": "",
    "text": "Image credits: Rod Searcey/Stanford HAI\nInstructions can be found at Learning from Timnit Gebru."
  },
  {
    "objectID": "posts/dr-gebru-talk/index.html#about-dr.-gebru",
    "href": "posts/dr-gebru-talk/index.html#about-dr.-gebru",
    "title": "Talk by Dr. Gebru",
    "section": "About Dr. Gebru",
    "text": "About Dr. Gebru\n\nTimnit Gebru ’08 M.A. ’10 Ph.D. ’17, a leader of the movement for diversity in tech and artificial intelligence, spoke about the dangers of AI at a Symbolic Systems department-sponsored event on Wednesday night. Gebru is the Founder and Executive Director of the Distributed Artificial Intelligence Research Institute (DAIR) and co-founder of the nonprofit to help with visibility and inclusion for Black people, Black in AI.\n\n— ‘Utopia for Whom?’: Timnit Gebru on the dangers of Artificial General Intelligence\nDr. Gebru, a distinguished computer scientist and influential advocate for diversity, will be giving a virtual talk at Middlebury on April 24th, 2023. She will also be virtually joining our Machine Learning class.\nDr. Gebru’s many works include:\n\nUsing deep learning and Google Street View to estimate the demographic makeup of neighborhoods across the United States derives estimates for income, race, education, and even voting patterns, from car images.\nGender shades: Intersectional accuracy disparities in commercial gender classification evaluates commercial gender classification systems to find an error rate gap of 34% between darker-skinned females and lighter-skinned males.\nOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜 covers the risk of very large language models, including BERT, GPT-2/3, and Switch-C.\n\nThen technical co-lead of the Ethical Artificial Intelligence Team, Google asked Dr. Gebru to either withdraw this paper or remove Google employees’ names from it. When she asked for reasons and advice, Google terminated her employment.\n\n\nHer long-standing research in AI, ethics, and equality, along with her active presence in podcasts and social media, has made Dr. Gebru an influential voice in the field."
  },
  {
    "objectID": "posts/dr-gebru-talk/index.html#fate-in-computer-vision",
    "href": "posts/dr-gebru-talk/index.html#fate-in-computer-vision",
    "title": "Talk by Dr. Gebru",
    "section": "FATE in Computer Vision",
    "text": "FATE in Computer Vision\nThis is a previous talk from CVPR 2020’s Tutorial on Fairness Accountability Transparency and Ethics in Computer Vision.\nWebsite, Video, Slides\n\nThe Field\nThe field of computer vision has harmed the Black community through policing, racial profiling, and other oppressive uses. It also has extremely few Black researchers, and less progress in equality and diversity has been made in this field than some other fields, like machine learning.\nPeople from Stanford perceived the use of Computer Vision in self-driving cars, policing, and education. But the reality is much darker, unfolding when our subjects are humans, i.e. facial recognition.\n\n\nFacial Recognition\nAs soon as we begin to investigate humans as the subject, Computer Vision becomes the propagator for systematic racism. Two companies using facial recognition are mentioned:\n\nFaception: profiling people with tags like “high IQ” and “terrorist” based only on their face\nHireVue: ATS for resume, facial and emotion analysis for job applications\n\nEach of these example demonstrate immediate dangers of amplifying existing bias, or introducing more. Personally, I think another danger is the potential justification through obfuscation, i.e. “we used this model and it performs well” to justify the system and hide the model’s biases (through the training data, or the model itself). This can even potentially conceal violations of relevant regulation such as equal opportunity laws, because investigation will be hard, and intent difficult to prove.\nAdding to the equation is the lack of regulation in AI and CV - there are no safety tests. AI models endure minimal scrutiny, especially when compared to the FDA approving drugs, whereas the impact can be similar.\n\n\nAbstraction\nMaking people disappear into mathematical equations, data points, models…\nGender recognition APIs (another layer of abstraction) had significantly higher error rates for Black women, as high as 46.8%. But the three datasets used to do this analysis (LFW, IJB-A, Adience) also had a disproportionate amount of around 80% lighter-skinned people.\nLack of diversity is not AI-specific - in crash tests for cars, for example, the dummies are based on the average male, putting female lives at risk. Even pharmaceuticals are being tested by White people and males more.\n\n\nVisibility vs. Inclusion\nWe cannot ignore social and structural problems.\nData were scraped from minorities (darker skin, transgender) without their consent, even though the intention might be to build more inclusive datasets.\nGender classification itself is problematic as well - potential issues for advocating gender conformity, etc.\n\n\nA Troubled System\nWide spread of systems for policing, surveillance, etc.:\n\nUS adults indexed in facial recognition: 1 out of 2\nRegulation: unregulated\nAccuracy and bias: unaudited\n\nAdditionally, these systems:\n\nTarget the marginalized\nDo not benefit the marginalized\nAre not built by the marginalized\n\n\n\nSelf-defense\nDefending oneself against these systems go beyond the traditional advice of “use a VPN” or “use Tor”. To counter the system, researchers propose fashion items that covers the face, long hair to cover the face, or infrared LEDs that disrupt cameras.\n\n\nTakeaway\nThe tl;dr for that everyone needs to understand about CV today:\nCV today amplifies systematic racism and bias, inflicting real harm on communities; not enough work is being done to mitigate this, let alone compensate."
  },
  {
    "objectID": "posts/dr-gebru-talk/index.html#eugenics-and-the-promise-of-utopia-through-agi",
    "href": "posts/dr-gebru-talk/index.html#eugenics-and-the-promise-of-utopia-through-agi",
    "title": "Talk by Dr. Gebru",
    "section": "Eugenics and the Promise of Utopia through AGI",
    "text": "Eugenics and the Promise of Utopia through AGI\nA recording from IEEE SaTML 2023 of a identically titled talk by Dr. Gebru:\nEugenics and the Promise of Utopia through AGI\n\nProposed Question\nMy proposed question for Dr. Gebru is:\n\nPeople have said things like “intellegent”, “creative”, and “artistic” about recent text and image generation models.\nThrough engaging with AI systems that can, say, recognize an expression or write like Shakespeare, where do you think is the goalpost for creativity? Do we determine that through the outcome, like a Turing test, or the process and models used? How do we evaluate such statements?\n\n\n\nThe Argument\n\nUtopia\nOrganizations like OpenAI promises that AI, more specifically AGI, will make for a bright future (Utopia).\nThe term AGI is not well-defined; after listing numerous definitions from different organizations, Dr. Gebru states that AGI sounds like “God can do anything”.\nBut current AI (potentially AGI) offerings exploited cheap labor, Mechanical Turk style. OpenAI exploited Kenyan workers to filter out toxic content for less than $2 per hour, leaving many with trauma and PTSD. Worse, these workers’ jobs are being replaced by the very AI systems they are being exploited to develop.\nThe question is thus:\n\nFor whom?\n\n\n\nFirst-wave Eugenics\nMostly “negative” eugenics, i.e. expel the bad.\nEugenics back in the day was protrayed as a scientific method, suggesting keywords like progressive, empowering, and technology.\nHowever, the outcome was dark, evil, and cost many lives.\nThe idea of improving “the human stock” lead to expulsion of those deemed to be undesirable, the “defectives” and “feeble-minded”, based on IQ tests.\nSince mankind is to be improved genetically, being poor was the result of one’s inferior nature, resulting in racism and white superiority as well.\n\n\nSecond-wave Eugenics\nMostly “positive” eugenics, i.e. improve to become the good.\nArising from the 1990s, the second wave of eugenics sought to use genetic engineering and biotechnology.\nWhat does eugenics have to do with AGI? The answer lies in the proponents of AGI, the “TESCREAL bundle”, that these ideologies are actually “second-wave eugenics”.\nDr. Gebru continues to show that the TESCREAL bundle has ideologies of “transcending the human race” and so on, which aligns them with eugenics.\nThese ideologies also contain elements of discrimination (against those unwilling to join them), an imagined (AI) utopia, and eschatology with convictions of an AGI apocalypse.\nOne example Dr. Gebru pointed out is how the “Effective Altruism” movement used IQ to score people. The metric is called PELTIV (Potential Expected Long-Term Instrumental Value), but is at heart a discriminatory view of individuals based on IQ.\nAdditionally, influential billionaires are in these movements, and they have enough funding to actually impact the world.\n\n\nDo one thing, not everything\nDr. Gebru compares Meta AI’s AI translation model, NLLB-200, with Lesan AI’s specialized model.\nWhile Lesan AI performed better, Meta’s announcement of an all-encompassing AI translator model made Lesan AI’s investors doubt its necessity.\nThis is one example of a “general” AI model harming smaller, specific models through investment pressure, and harming the minorities they represent as well.\nAlthough not emphasized, Dr. Gebru’s takeaway message was, instead of building AGIs, build specific systems with clearly-defined scopes."
  },
  {
    "objectID": "posts/dr-gebru-talk/index.html#reflections",
    "href": "posts/dr-gebru-talk/index.html#reflections",
    "title": "Talk by Dr. Gebru",
    "section": "Reflections",
    "text": "Reflections\nI feel priviledged to be able to attend this talk.\nThe talk was based on a yet-to-be published paper, so instead of reading a paper, I had to look for tweets and forum discussions to learn more.\nAround the same time as researching for this blog post, I came across a YouTube video titled The Rich Have Their Own Ethics: Effective Altruism & the Crypto Crash. The title echoes one of the takeaways from Dr. Gebru’s talk, that the rich needed their own ethics to convince themselves they are doing good things.\n\nForums\nI also dug around the Effective Altruism forums a little bit.\n\nMosquito nets\nPublished back in 2015, this The Atlantic article portrays effective altruism as a positive force in helping end malaria with mosquito nets. In the forum however, there was a heated discussion about whether to donate mosquito nets or not, because people were unsure about the long-term benefit of doing so. (In the short term, they agree the consequences are “unambiguously positive”.)\nTo me, this suggests an unhealthy bias towards thingking long-term. Additionally, given that there are many more people in the future, an action that benefits the future but has short-term consequences might prompt one to ignore the latter.\n\n\nIQ scores\nWith regard to scoring people by IQ, the only criticism I encountered in the forum was that keeping personal records violated GDPR, a data protection law.\nAnother concern on recording IQ scores was dismissed because the practice of keeping tags on people was so common, and such behaviour is creppy “by design”.\n\n\nOn Dr. Gebru\nDr. Gebru was mentioned once when the SaTML talk was posted; the poster specifically advised viewers to ignore criticism towards the EA community.\nAnother post asked for advice on engaging with people like Dr. Gebru. Interestingly, the poster acknowledges their work on social justice, but dismisses these worries with “Algorithmic bias today is not the same as x-risk from unaligned AI in 30 years.” (human extinction risk?) The post and comment section, however, were unconvinced and somewhat adversarial. This conforms to what one of Dr. Gebru’s collaborators has characterized as “cultish behavior”.\n\n\n\nOn Campus\nThe Effective Altruism movement has had a visible on-campus presence for quite a while now.\nOne of their posters illustrate doing good effectively; another is a reading group on the dangers of AGI.\nAfter the talk, my (unqualified and unasked for) advice to them is twofolds:\n\nFocus on one thing at a time, like choosing a charity to donate to, or tips on buying more sustainably.\n\nTrying to maximize utility, for the good of all mankind, for the long term, and avoiding an AGI apocalypse all at once might lead to harmful results.\n\nFind a new name, given the declining reputation of the Effective Altruism movement.\n\nI trust their intention to make the world a better place, and I also don’t think they will actively engage in something that is harmful for the present but beneficial to the future.\nTherefore, associating themselves with “rich people wanting their own ethics” might not be the best idea."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\LaTeX markup language. For example, $f(x) \\approx y$ renders to f(x) \\approx y. To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/linear-regression/index.html",
    "href": "posts/linear-regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Image credits: Figure 18.3, Sanchez, G., Marzban, E. (2020) All Models Are Wrong: Concepts of Statistical Learning.\nPython source: 451-blog/linreg.py at main · doabell/451-blog\nInstructions can be found at Implementing Linear Regression."
  },
  {
    "objectID": "posts/linear-regression/index.html#algorithm-demo",
    "href": "posts/linear-regression/index.html#algorithm-demo",
    "title": "Linear Regression",
    "section": "Algorithm Demo",
    "text": "Algorithm Demo\nWe implement linear regression in two methods:\n\nUsing the analytical formula for the optimal weight vector\nUsing gradient descent on the gradient formula\n\n\nData\n\n# Data\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib import font_manager\nfont_manager.fontManager.addfont(\"C:\\Windows\\Fonts\\FiraSans-Regular.ttf\")\nplt.rcParams[\"font.family\"] = \"Fira Sans\"\n\nnp.random.seed(283)\n\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\n\ndef LR_data(n_train=100, n_val=100, p_features=1, noise=.1, w=None):\n    if w is None:\n        w = np.random.rand(p_features + 1) + .2\n\n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n\n    return X_train, y_train, X_val, y_val\n\n\n# Visualize\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.16\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex=True, sharey=True)\naxarr[0].scatter(X_train, y_train, alpha=0.5)\naxarr[1].scatter(X_val, y_val, alpha=0.5)\nlabs = axarr[0].set(title=\"Training\", xlabel=\"x\", ylabel=\"y\")\nlabs = axarr[1].set(title=\"Validation\", xlabel=\"x\")\nplt.tight_layout()\n\n\n\n\n\n\nAnalytical\nWe call LR.fit(), which defaults to using the analytical formula:\n\n# Train model\nfrom linreg import LinearRegression\n\nLR = LinearRegression()\nLR.fit(X_train, y_train)\n\nfig, axarr = plt.subplots(1, 2, sharex=True, sharey=True)\naxarr[0].scatter(X_train, y_train, alpha=0.5)\naxarr[0].plot(X_train, pad(X_train)@LR.w, color=\"black\")\naxarr[1].scatter(X_val, y_val, alpha=0.5)\naxarr[1].plot(X_val, pad(X_val)@LR.w, color=\"black\")\nlabs = axarr[0].set(title=\"Training\", xlabel=\"x\", ylabel=\"y\")\nlabs = axarr[1].set(title=\"Validation\", xlabel=\"x\")\nplt.tight_layout()\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.7162\nValidation score = 0.7064\n\n\n\n\n\nOur validation score is similar to the training score.\nVisually, the resulting line looks like a good fit for the data.\nWe can then check our estimated weight vector:\n\n# Estimated weight vector\nLR.w\n\narray([0.80273893, 0.49734664])\n\n\n\n\nGradient descent\nWe call linear regression with method=\"gradient\":\n\n# Using gradient descent\nLR2 = LinearRegression()\n\nLR2.fit(X_train, y_train, method=\"gradient\", alpha=0.001, max_iter=100)\nprint(f\"Training score = {LR2.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR2.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.6433\nValidation score = 0.6966\n\n\nThe scores are similar to that of the analytical method.\nWe can check how the training score evolved with each iteration:\n\n# score history\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel=\"Iteration\", ylabel=\"Score\")\n\n\n\n\nWe observe that the score improved quickly to around 0.6, and then improved more slowly.\nFinally, we can plot the resulting regression lines:\n\nfig, axarr = plt.subplots(1, 2, sharex=True, sharey=True)\naxarr[0].scatter(X_train, y_train, alpha=0.5)\naxarr[0].plot(\n    X_train, pad(X_train)@LR.w,\n    label=\"Analytical\", alpha=0.7, color=\"orange\"\n)\naxarr[0].plot(\n    X_train, pad(X_train)@LR2.w,\n    label=\"Gradient descent\", alpha=0.7, color=\"green\"\n)\naxarr[1].scatter(X_val, y_val, alpha=0.5)\naxarr[1].plot(\n    X_val, pad(X_val)@LR.w,\n    label=\"Analytical\", alpha=0.7, color=\"orange\"\n)\naxarr[1].plot(\n    X_val, pad(X_val)@LR2.w,\n    label=\"Gradient descent\", alpha=0.7, color=\"green\"\n)\nlabs = axarr[0].set(title=\"Training\", xlabel=\"x\", ylabel=\"y\")\nlabs = axarr[1].set(title=\"Validation\", xlabel=\"x\")\nplt.tight_layout()\nlegend = plt.legend()\n\n\n\n\nAlthough the two lines are slightly different, they are both close enough visually."
  },
  {
    "objectID": "posts/linear-regression/index.html#experiments",
    "href": "posts/linear-regression/index.html#experiments",
    "title": "Linear Regression",
    "section": "Experiments",
    "text": "Experiments\nWe now experiment with the number of features used in our linear regression model.\nMore features may lead to better scores, but might also make the model overfit.\n\nAnalytical\nFirst, we make some data with 1 to 99 features. Then, we can fit our analytical linear regression on the data as follows:\n\nscores_train_ana = []\nscores_val_ana = []\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\nfor p_features in range(1, n_train):\n    X_train, y_train, X_val, y_val = LR_data(\n        n_train, n_val, p_features, noise\n    )\n    LR = LinearRegression()\n    LR.fit(X_train, y_train)\n    scores_train_ana.append(LR.score(X_train, y_train))\n    scores_val_ana.append(LR.score(X_val, y_val))\n\n# https://stackoverflow.com/a/67037892\nplt.plot(range(1, len(scores_train_ana) + 1),\n         scores_train_ana, label=\"Training\")\nplt.plot(range(1, len(scores_val_ana) + 1), scores_val_ana, label=\"Validation\")\nlabels = plt.gca().set(xlabel=\"Number of features\", ylabel=\"Score\")\nlegend = plt.legend()\n\n\n\n\nAs we increase the number of features to fit on, the training accuracy increases because more features are available. The improvements beyond 20 features is very small though.\nThe validation score, however, decrease as we fit over more features. It even drops below 0 when the number of features gets close to 100 (the number of training and validation data points)!\nHere, we can see overfitting going on. Especially in the case of fitting over 80 features on 100 data points, in the worst-case scenario, each data point can have a fairly distinct set of feature combinations, leading to overfitting. This is despite our data being generated with an underlying distribution (as seen in LR_data()).\n\n\nGradient descent\nWe can repeat the same experiment, but for gradient descent.\n\nscores_train_gra = []\nscores_val_gra = []\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\nplt.figure(1)\nfor p_features in range(1, n_train):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n    LR = LinearRegression()\n    LR.fit(X_train, y_train, method=\"gradient\", alpha=0.0001, max_iter=2000)\n    scores_train_gra.append(LR.score(X_train, y_train))\n    scores_val_gra.append(LR.score(X_val, y_val))\n    # plot score over history\n    if p_features in [1, 5, 10, 25, 50, 99]:\n        plt.plot(\n            LR.score_history,\n            label=f\"{p_features} features\",\n            alpha=0.6\n        )\nlabels = plt.gca().set(xlabel=\"Iteration\", ylabel=\"Score\")\nlegend = plt.legend()\nplt.ylim(bottom=0.5, top=1)\n\n# plot score over features\nplt.figure(2)\nplt.plot(\n    range(1, len(scores_train_gra) + 1),\n    scores_train_gra, label=\"Training\", alpha=0.7\n)\nplt.plot(\n    range(1, len(scores_val_gra) + 1),\n    scores_val_gra, label=\"Validation\", alpha=0.7\n)\nlabels = plt.gca().set(xlabel=\"Number of features\", ylabel=\"Score\")\nlegend = plt.legend()\n\n\n\n\n\n\n\nFor score based on the number of features, we observe a similar trend from using the analytical formula. There is not much improvement beyond 20 features, and the validation score fluctuates a lot with a higher number of features.\nAlso of interest is how the training score changes over each iteration, with different number of features:\n\nWhen there is only 1 feature, our training score does not get higher than 0.7.\nThe more features, we use, the higher training score we get (as illustrated by the second plot)\nHowever, when only using 10 features, we already achieve a training score higher than 0.9.\nHigher number of features yield smaller improvements.\n\nThis behavior is consistent with overfitting, where the training score can get really close to 1, but the validation score fluctuates.\n\n\nLASSO Regularization\nOne way to deal with this sort of overfitting, or indeed the presence of too many features, is to use a feature selection algorithm.\nHere, we use LASSO regularization implemented in the scikit-learn package. LASSO has a tendency to give zero weights to features, making for feature selection.\nLASSO has a hyperparameter, alpha, that controls the strength of regularization. This can be anywhere in [0, inf).\nWe repeat the above experiment to see how LASSO regularization can guard against overfitting.\n\nfrom sklearn.linear_model import Lasso\n\nscores_train_las = []\nscores_val_las = []\n\nn_train = 100\nn_val = 100\nnoise = 0.2\n\nfor p_features in range(1, n_train):\n    X_train, y_train, X_val, y_val = LR_data(\n        n_train, n_val, p_features, noise\n    )\n    L = Lasso(alpha=0.001)\n    L.fit(X_train, y_train)\n    scores_train_las.append(L.score(X_train, y_train))\n    scores_val_las.append(L.score(X_val, y_val))\n\n\n# Plot\nfig, axarr = plt.subplots(1, 2, sharex=True, sharey=True)\naxarr[0].plot(\n    range(1, len(scores_train_las) + 1), scores_train_las,\n    label=\"Training\", alpha=0.7\n)\naxarr[0].plot(\n    range(1, len(scores_val_las) + 1), scores_val_las,\n    label=\"Validation\", alpha=0.7\n)\naxarr[1].plot(\n    range(1, len(scores_train_ana) + 1), scores_train_ana,\n    label=\"Training\", alpha=0.7\n)\naxarr[1].plot(\n    range(1, len(scores_val_ana) + 1), scores_val_ana,\n    label=\"Validation\", alpha=0.7\n)\nlabs = axarr[0].set(title=\"LASSO\", xlabel=\"Number of features\", ylabel=\"Score\")\nlabs = axarr[1].set(title=\"Analytical\", xlabel=\"Number of features\")\nplt.tight_layout()\nlegend = plt.legend()\n\n\n\n\nWith a large number of features, LASSO was successful in reducing the training-validation gap that indicates overfitting.\nThat said, the validation score still drops noticably for over 80 features. Thus, for overparameterized problems like this where the number of features is close to (or greater than!) the number of data points, it is still more advisable to:\n\nfit with less features, or\nreduce the severity of overparameterization\n\nAdditionally, LASSO also performs better with few (around 1-5) features, but this is subject to randomness (like gradient descent)."
  },
  {
    "objectID": "posts/linear-regression/index.html#data-bike-sharing",
    "href": "posts/linear-regression/index.html#data-bike-sharing",
    "title": "Linear Regression",
    "section": "Data: Bike Sharing",
    "text": "Data: Bike Sharing\nNext, we can test the real-life performance of linear regression, with the analytical formula or with gradient descent.\nWe will be using this bike-sharing dataset, from Washington DC1, to predict the number of casual bikeshare users given info on that day.\n(The setup code below is adapted from the blog post instructions.)\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nbikeshare = pd.read_csv(\n    \"https://philchodrow.github.io/PIC16A/datasets/Bike-Sharing-Dataset/day.csv\")\n\ncols = [\"casual\",\n        \"mnth\",\n        \"weathersit\",\n        \"workingday\",\n        \"yr\",\n        \"temp\",\n        \"hum\",\n        \"windspeed\",\n        \"holiday\",\n        \"dteday\"]\n\nbikeshare_plot = bikeshare[cols]\nbikeshare_plot = pd.get_dummies(bikeshare_plot, columns=[\n                                'mnth'], drop_first=\"if_binary\")\n\ntrain, test = train_test_split(bikeshare_plot, test_size=.2, shuffle=False)\n\nX_train = train.drop([\"casual\", \"dteday\"], axis=1)\ny_train = train[\"casual\"]\n\nX_test = test.drop([\"casual\", \"dteday\"], axis=1)\ny_test = test[\"casual\"]\n\nX_train.head()\n\n\n\n\n\n\n\n\nweathersit\nworkingday\nyr\ntemp\nhum\nwindspeed\nholiday\nmnth_2\nmnth_3\nmnth_4\nmnth_5\nmnth_6\nmnth_7\nmnth_8\nmnth_9\nmnth_10\nmnth_11\nmnth_12\n\n\n\n\n0\n2\n0\n0\n0.344167\n0.805833\n0.160446\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n2\n0\n0\n0.363478\n0.696087\n0.248539\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n1\n0\n0.196364\n0.437273\n0.248309\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n1\n1\n0\n0.200000\n0.590435\n0.160296\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n1\n1\n0\n0.226957\n0.436957\n0.186900\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nX_train.shape\n\n(584, 18)\n\n\n\nAnalytical\nWe first use the analytical method, inspecting the scores and weight vectors:\n\nLR4 = LinearRegression()\nLR4.fit(X_train, y_train)\n\nprint(f\"Training score = {LR4.score(X_train, y_train).round(6)}\")\nprint(f\"Test score = {LR4.score(X_test, y_test).round(6)}\")\n\nTraining score = 0.731836\nTest score = 0.696773\n\n\n\npd.Series(LR4.w[:-1], index=X_train.columns).sort_values()\n\nwindspeed    -1242.800381\nworkingday    -791.690549\nhum           -490.100340\nholiday       -235.879349\nweathersit    -108.371136\nmnth_2          -3.354397\nmnth_12         90.821460\nmnth_7         228.881481\nmnth_8         241.316412\nmnth_11        252.433004\nyr             280.586927\nmnth_6         360.807998\nmnth_3         369.271956\nmnth_9         371.503854\nmnth_10        437.600848\nmnth_4         518.408753\nmnth_5         537.301886\ntemp          1498.715113\ndtype: float64\n\n\n\n\nGradient Descent\nThen, we repeat the same process with gradient descent.\n\nLR5 = LinearRegression()\nLR5.fit(X_train, y_train, method=\"gradient\", alpha=0.0001, max_iter=10000)\n\nprint(f\"Training score = {LR5.score(X_train, y_train).round(6)}\")\nprint(f\"Test score = {LR5.score(X_test, y_test).round(6)}\")\n\nTraining score = 0.731833\nTest score = 0.696753\n\n\n\npd.Series(LR5.w[:-1], index=X_train.columns).sort_values()\n\nwindspeed    -1230.491728\nworkingday    -791.472877\nhum           -484.647522\nholiday       -235.399837\nweathersit    -109.260413\nmnth_2          -1.748008\nmnth_12         93.293066\nmnth_7         235.754820\nmnth_8         247.603486\nmnth_11        255.436488\nyr             281.231420\nmnth_6         366.826944\nmnth_3         371.749383\nmnth_9         376.868083\nmnth_10        441.334934\nmnth_4         521.567646\nmnth_5         541.885682\ntemp          1488.043873\ndtype: float64\n\n\nWe achieved nearly identical results with the two methods.\nSince we have reasonable numbers of data points (584) and features (18), we do expect that the analytical and gradient descent methods to arrive at a similarly good goal.\n\n\nLASSO Regularization\n\nL2 = Lasso(alpha=0.05)\nL2.fit(X_train, y_train)\nprint(f\"Training score = {L2.score(X_train, y_train).round(6)}\")\nprint(f\"Test score = {L2.score(X_test, y_test).round(6)}\")\n\nTraining score = 0.731824\nTest score = 0.696659\n\n\n\npd.Series(L2.coef_, index=X_train.columns).sort_values()\n\nwindspeed    -1233.278124\nworkingday    -791.462521\nhum           -484.813541\nholiday       -234.619924\nweathersit    -108.888907\nmnth_2          -8.285024\nmnth_12         83.321973\nmnth_7         214.900960\nmnth_8         226.955076\nmnth_11        243.204653\nyr             279.195051\nmnth_6         348.087677\nmnth_9         358.328354\nmnth_3         361.587089\nmnth_10        427.148731\nmnth_4         509.108560\nmnth_5         525.695908\ntemp          1516.891474\ndtype: float64\n\n\nYet again, we arrive at similar results. This indicates that there is no overparameterization in this dataset, like the previous experiment.\n\n\nFeature weights\nSo what do these numbers actually mean? Features with positive numbers contribute to (casual) ridership; features with negative numbers, surprisingly, also contribute to ridership, but negatively.\nReading off the individual labels:\n\nThe higher the average temperature, the more bike riders. This metric might be more helpful if we classified temperatures as “too cold”, “too hot”, and “great for biking”.\nPeople biked more in March, April, May, and September. This might also have to do with the temperature, or nice weather in general.\nPeople biked less if it is a workingday, and biked more at weekends.\nPeople biked less if there is a strong wind.\n\n\n\nPredictions\nWe first plot out the original, complete data for reference:\n\nfig, ax = plt.subplots(1, figsize=(7, 3))\nax.plot(\n    pd.to_datetime(bikeshare['dteday']),\n    bikeshare['casual'], alpha=0.6\n)\nax.set(xlabel=\"Day\", ylabel=\"# of casual users\")\nl = plt.tight_layout()\n\n\n\n\nThis is important because the next plot is of the test set, and is therefore not continuous in dates.\nNext, we can plot our predictions against the actual riderships. Here, LR5, our linear regression model with gradient descent, is used as the predictor.\n\n# import datetime\nfig, ax = plt.subplots(1, figsize=(8, 3))\nax.plot(\n    pd.to_datetime(test['dteday']),\n    test['casual'],\n    label=\"Actual\",\n    alpha=0.6\n)\nax.plot(\n    pd.to_datetime(test['dteday']),\n    LR5.predict(X_test),\n    label=\"Predicted\",\n    alpha=0.6\n)\nax.set(xlabel=\"Day (from test set)\", ylabel=\"# of casual users\")\nplt.tight_layout()\nlegend = plt.legend()\n\n\n\n\nWe observe that the predicted ridership is much smoother than the actual riderships.\nThis may be due to our features not capturing enough real-life conditions, and / or due to the original data varying a lot day by day.\nHowever, our model was able to predict the “shape” of casual ridership changes, so the general direction of things is there."
  },
  {
    "objectID": "posts/linear-regression/index.html#footnotes",
    "href": "posts/linear-regression/index.html#footnotes",
    "title": "Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFanaee-T, H., Gama, J. Event labeling combining ensemble detectors and background knowledge. Prog Artif Intell 2, 113–127 (2014). https://doi.org/10.1007/s13748-013-0040-3↩︎"
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Optimization for Logistic Regression",
    "section": "",
    "text": "Image credit: European Union, Economies of scope in the aggregation of health-related data, under CC BY 4.0.\nPython source: 451-blog/logreg.py at main · doabell/451-blog\nInstructions can be found at Optimization for Logistic Regression."
  },
  {
    "objectID": "posts/logistic-regression/index.html#comparing-gradient-descents",
    "href": "posts/logistic-regression/index.html#comparing-gradient-descents",
    "title": "Optimization for Logistic Regression",
    "section": "Comparing gradient descents",
    "text": "Comparing gradient descents\nThe Python source file implements three types of logistic regression with gradient descent:\n\nBatch gradient descent, updating the gradient all at once;\nStochastic (minibatch) gradient descent, updating the gradient on random subsets;\nStochastic (minibatch) gradient descent with momentum method.\n\n\nNote on terminology\nI checked Google for a reasonable batch_size for this blog post’s “stochastic gradient descent”, and was unable to find one.\nFrom this Towards Data Science blog post, it seems that “stochastic gradient descent” refers to an online method that checks one data point at a time. A more common name for the method implemented in LR.fit_stochastic() is “minibatch” gradient descent, which strikes a balance between stochastic (one at a time) and “batch” (LR.fit()) gradient descent (all the data points, all at once).\n\n\nBatch gradient descent\nWith terminology out of the way, let us first prepare the data.\nWe generate data points with 2 features and 2 labels that are not linearly separable, as seen below:\n\n# Data\nimport numpy as np\nfrom logreg import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nfrom matplotlib import font_manager\nfont_manager.fontManager.addfont(\"C:\\Windows\\Fonts\\FiraSans-Regular.ttf\")\nplt.rcParams[\"font.family\"] = \"Fira Sans\"\nnp.seterr(all='ignore')\nnp.random.seed(283)\n\n# make the data\np_features = 3\nX, y = make_blobs(\n    n_samples=500, n_features=p_features - 1, centers=[(-1, -1), (1, 1)]\n)\n\nfig = plt.scatter(X[:, 0], X[:, 1], c=y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can then fit our logistic regression with batch gradient descent as follows:\n\nLR_batch = LogisticRegression()\nLR_batch.fit(X, y, alpha=0.1, max_epochs=1000)\n\nfig, axarr = plt.subplots(1, 2)\n\naxarr[0].scatter(X[:, 0], X[:, 1], c=y)\naxarr[0].set(\n    xlabel=\"Feature 1\", ylabel=\"Feature 2\",\n    title=f\"Loss = {LR_batch.loss_history[-1]}\"\n)\n\nf1 = np.linspace(-3, 3, 101)\n\np = axarr[0].plot(\n    f1,\n    (LR_batch.w[2] - f1*LR_batch.w[0])/LR_batch.w[1],\n    color=\"black\"\n)\n\naxarr[1].plot(LR_batch.loss_history)\naxarr[1].set(xlabel=\"Iteration number\", ylabel=\"Empirical Risk\")\nplt.tight_layout()\n\n\n# inspect the fitted value of w\nLR_batch.w\n\narray([ 1.82323946,  1.7608397 , -0.10917852])\n\n\n\n\n\nWe observe a resonable splitting line on the left, and a smoothly decreasing loss to the right.\nSince we also implemented stochastic “minibatch” gradient descent, with and without momentum, we can plot the loss curves together as follows: (Code adapted from blog post instructions)\n\nLR = LogisticRegression()\nLR.fit_stochastic(\n    X, y,\n    max_epochs=1000,\n    momentum=True,\n    batch_size=10,\n    alpha=.01\n)\n\nnum_steps = len(LR.loss_history)\nplt.plot(\n    np.arange(num_steps) + 1, LR.loss_history,\n    label=\"stochastic gradient (momentum)\"\n)\n\nLR = LogisticRegression()\nLR.fit_stochastic(\n    X, y,\n    max_epochs=1000,\n    momentum=False,\n    batch_size=10,\n    alpha=.01\n)\n\nnum_steps = len(LR.loss_history)\nplt.plot(\n    np.arange(num_steps) + 1, LR.loss_history,\n    label=\"stochastic gradient\"\n)\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha=.01, max_epochs=1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(\n    np.arange(num_steps) + 1,\n    LR.loss_history,\n    label=\"gradient\"\n)\n\nplt.loglog()\n\nlegend = plt.legend()\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nWe can see different converging speeds for the three types of gradient descent. Here, stochastic “minibatch” gradient descent converges the fastest, then “minibatch” with momentum (slightly slower), then standard gradient descent."
  },
  {
    "objectID": "posts/logistic-regression/index.html#experiments",
    "href": "posts/logistic-regression/index.html#experiments",
    "title": "Optimization for Logistic Regression",
    "section": "Experiments",
    "text": "Experiments\nHaving compared the different ways of gradient descent, we now investigate the hyperparameters. Namely:\n\nLearning rate \\alpha\nBatch size\nMomentum vs. regular “minibatch” stochastic gradient descent\n\n\nData\nWe will generate another dataset with 10 features, visualizing the first two:\n\n# make the data\np_features = 11\nX, y = make_blobs(\n    n_samples=500, n_features=p_features - 1, centers=[(-1, -1), (1, 1)]\n)\n\nfig = plt.scatter(X[:, 0], X[:, 1], c=y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\nLearning rate\nThe learning rate determines how much we move along the gradient.\nIn code , the relevant line, Line 110, reads:\nself.w = self.w - alpha * gradient\nAs such, when the learning rate is too large, gradient descent will exceed the optimal minimum; when it is too small, gradient descent will take longer to reach the optimal minimum.\n\nalphas = [0.05, 0.0005, 1]\n\nfor alpha in alphas:\n    LR = LogisticRegression()\n    LR.fit_stochastic(\n        X, y,\n        max_epochs=1000,\n        momentum=False,\n        batch_size=10,\n        alpha=alpha\n    )\n\n    num_steps = len(LR.loss_history)\n    plt.plot(\n        np.arange(num_steps) + 1, LR.loss_history,\n        label=f\"alpha = {alpha}\",\n        alpha=0.6 if alpha == 1 else 1\n    )\n\nplt.loglog()\n\nlegend = plt.legend()\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nWith three different learning rates, we can see a difference in how training loss progressed over time:\n\nReasonably small, alpha = 0.05: loss decreased quickly, converging in less than 100 iterations.\nToo small, alpha = 0.0005: loss decreased more smoothly but slowly, and failed to converge in 1000 iterations.\nToo large, alpha = 1: loss jumped up and down frequently, although it managed to converge in less than 100 iterations.\n\nThis shows that indeed, we want a reasonably small learning rate. Small enough to converge, but not too small that converging becomes too slow.\n\n\nBatch size\nWe look at another hyperparameter, the batch size.\nIn our “minibatch” gradient descent, we look at each “batch” separately. The size of these batches, therefore, might determine the speed of gradient descent.\nUsing our reasonable learning rate of alpha = 0.05:\n\nfrom time import perf_counter\nbatch_sizes = [2, 10, 50, 100, 200, 500]\n\nfor batch_size in batch_sizes:\n    LR = LogisticRegression()\n    start = perf_counter()\n    LR.fit_stochastic(\n        X, y,\n        max_epochs=1000,\n        momentum=False,\n        batch_size=batch_size,\n        alpha=0.05\n    )\n    end = perf_counter()\n\n    num_steps = len(LR.loss_history)\n    plt.plot(\n        np.arange(num_steps) + 1, LR.loss_history,\n        label=f\"batch size = {batch_size}, {(end - start)*1000:.2f}ms\",\n        alpha=0.7\n    )\n\nplt.loglog()\n\nlegend = plt.legend()\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nHere, using otherwise identical hyperparameters, we observe that batch size does affect convergence speed and runtime.\nAlthough a batch size of 2 converged the fastest, the program runtime was longer because of more calculations per loop.\nA batch size of 50 converged rather quickly, and had a decently short runtime as well.\nLarger batch sizes took longer to converge. At a batch size of 500, this is effectively regular gradient descent, the one callable with LR.fit().\nIn conclusion, a reasonable batch size would make gradient descent converge faster, and improve the program runtime as well.\n\n\nMomentum\n\nfor momentum in [False, True]:\n    LR = LogisticRegression()\n    start = perf_counter()\n    LR.fit_stochastic(\n        X, y,\n        max_epochs=100,\n        momentum=momentum,\n        batch_size=20,\n        alpha=0.5\n    )\n    end = perf_counter()\n\n    num_steps = len(LR.loss_history)\n    plt.plot(\n        np.arange(num_steps) + 1, LR.loss_history,\n        label=f\"momentum {momentum}, {(end - start)*1000:.2f}ms\",\n        alpha=0.7\n    )\n\nplt.loglog()\n\nlegend = plt.legend()\nxlab = plt.xlabel(\"Epoch\")\nylab = plt.ylabel(\"Loss\")\n\n\n\n\nWith a large-enough learning rate of 0.5, we observe the power of the momentum method in gradient descent.\nWhen using the momentum method, “minibatch” stochastic gradient descent converged in around 10 iterations. Not using the momentum method resulted in a much slower convergence."
  },
  {
    "objectID": "posts/penguins/index.html",
    "href": "posts/penguins/index.html",
    "title": "Penguins",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom matplotlib import font_manager\nfont_manager.fontManager.addfont(\"C:\\Windows\\Fonts\\FiraSans-Regular.ttf\")\nwarnings.filterwarnings('ignore')\n\nsns.set_theme()\nsns.set(font=\"Fira Sans\")\nImage credits: Artwork by @allison_horst. GitHub link\nInstructions can be found at Classifying Palmer Penguins."
  },
  {
    "objectID": "posts/penguins/index.html#explore",
    "href": "posts/penguins/index.html#explore",
    "title": "Penguins",
    "section": "Explore",
    "text": "Explore\nThe penguins dataset features measurements for three penguin species observed in the Palmer Archipelago, Antarctica (more information in link).\nInformation on the data contained: Penguin size, clutch, and blood isotope data for foraging adults near Palmer Station, Antarctica\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nLengths\nWe examine the first quantitative elements of the dataset - the penguins’ Culmen and Flipper lengths, and see how they relate to the penguins’ species.\n\n# wrap legend\nfrom textwrap import fill\ntrain_map = train.copy(deep=True)\ntrain_map[\"Species\"] = train_map[\"Species\"].apply(fill, width=20)\n\n\nlengthsPlot = sns.lmplot(\n    train_map,\n    x=\"Culmen Length (mm)\",\n    y=\"Flipper Length (mm)\",\n    hue=\"Species\"\n)\n\n\n\n\nThese are three pretty distinct clusters, though with some overlap!\nAs such, we could probably say that the penguins with the smallest culmen and flipper lengths are probably Adelie penguins.\n\n\nBlood isotopes\nNext, we look at isotope data for these penguins:\n\nisotopesPlot = sns.relplot(\n    train_map, x=\"Delta 15 N (o/oo)\", y=\"Delta 13 C (o/oo)\", hue=\"Species\")\n\n\n\n\nThese clusters turned out to be pretty inseparable.\nDoes blood isotope data have to do with where these penguins live?\n\nisotopesIslandPlot = sns.relplot(\n    train_map, x=\"Delta 15 N (o/oo)\", y=\"Delta 13 C (o/oo)\", hue=\"Island\")\n\n\n\n\nThe answer is probably no; we see no patterns at all here.\n\n\nIsland\nSo where do these penguins live?\n\nlivePlot = sns.displot(train_map, x=\"Island\", y=\"Species\", aspect=1.4)\n\n\n\n\n\ntrain.groupby([\"Island\", \"Sex\", \"Species\"]).count()[[\"Region\"]]\n\n\n\n\n\n\n\n\n\n\nRegion\n\n\nIsland\nSex\nSpecies\n\n\n\n\n\nBiscoe\n.\nGentoo penguin (Pygoscelis papua)\n1\n\n\nFEMALE\nAdelie Penguin (Pygoscelis adeliae)\n19\n\n\nGentoo penguin (Pygoscelis papua)\n42\n\n\nMALE\nAdelie Penguin (Pygoscelis adeliae)\n16\n\n\nGentoo penguin (Pygoscelis papua)\n54\n\n\nDream\nFEMALE\nAdelie Penguin (Pygoscelis adeliae)\n20\n\n\nChinstrap penguin (Pygoscelis antarctica)\n29\n\n\nMALE\nAdelie Penguin (Pygoscelis adeliae)\n20\n\n\nChinstrap penguin (Pygoscelis antarctica)\n27\n\n\nTorgersen\nFEMALE\nAdelie Penguin (Pygoscelis adeliae)\n18\n\n\nMALE\nAdelie Penguin (Pygoscelis adeliae)\n19\n\n\n\n\n\n\n\nWe observe that, Gentoo penguins only live on the Biscoe Islands, and Chinstrap penguins on Dream Island; Adelie penguins happily (or at least hopefully happily) live on all three islands.\nAlso, Torgensen Island only has Adelie penguins; the other islands have at least two species.\nLooking at sex, the penguins on each island are pretty evenly split between male and female.\nNote this might only be specific to our test-train split, so we need to be cautious of not over-fitting - we cannot say, for example, that a penguin on Torgensen Island is 100% Adelie."
  },
  {
    "objectID": "posts/penguins/index.html#model",
    "href": "posts/penguins/index.html#model",
    "title": "Penguins",
    "section": "Model",
    "text": "Model\n\nPrepare data\nWe prepare Species as labels, and then other features with pd.get_dummies().\nRows with invalid Sex fields or fields with NA are dropped.\nSeveral identifying columns, like Individual ID, are also dropped.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n\ndef prepare_data(df):\n    df = df.drop(\n        [\"studyName\", \"Sample Number\", \"Individual ID\",\n         \"Date Egg\", \"Comments\", \"Region\"], axis=1\n    )\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis=1)\n    df = pd.get_dummies(df)\n    return df, y\n\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n1\n45.1\n14.5\n215.0\n5000.0\n7.63220\n-25.46569\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n2\n41.4\n18.5\n202.0\n3875.0\n9.59462\n-25.42621\n0\n0\n1\n1\n0\n1\n0\n1\n\n\n3\n39.0\n18.7\n185.0\n3650.0\n9.22033\n-26.03442\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n4\n50.6\n19.4\n193.0\n3800.0\n9.28153\n-24.97134\n0\n1\n0\n1\n1\n0\n0\n1\n\n\n5\n33.1\n16.1\n178.0\n2900.0\n9.04218\n-26.15775\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n\n\n\n\n\n\n\nFeature selection\nWe need to select 3 features for our classification model.\nSince one of these need to be qualitative, we may have to look at more than 3.\nFrom scikit-learn, 1.13. Feature Selection:\n\nUnivariate statistical tests\nWe can apply univariate statistical tests to find the statistically “best” features.\nFor classification, we have three scores available:\n\nchi2 is for contigency tables, or at least non-negative values. Since the column Delta 13 C (o/oo) contains negative values, we cannot use this score.\nf_classif computes the ANOVA F-value.\nmutual_info_classif estimates mutual information for a discrete target variable.\n\nWe use SelectKBest to remove all but the best K features:\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\nselector = SelectKBest(f_classif, k=3)\nX_new = selector.fit_transform(X_train, y_train)\n# scikit-learn does not keep names ):\n# https://stackoverflow.com/a/41041230\nX_train.loc[:, selector.get_support()].head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n1\n45.1\n215.0\n5000.0\n\n\n2\n41.4\n202.0\n3875.0\n\n\n3\n39.0\n185.0\n3650.0\n\n\n4\n50.6\n193.0\n3800.0\n\n\n5\n33.1\n178.0\n2900.0\n\n\n\n\n\n\n\nWe see that these three are all quantitative.\nTherefore, we keep the top 2:\n\nquant_selector = SelectKBest(f_classif, k=2)\nX_new = quant_selector.fit_transform(X_train, y_train)\n# scikit-learn does not keep names ):\n# https://stackoverflow.com/a/41041230\nX_train.loc[:, quant_selector.get_support()].head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nFlipper Length (mm)\n\n\n\n\n1\n45.1\n215.0\n\n\n2\n41.4\n202.0\n\n\n3\n39.0\n185.0\n\n\n4\n50.6\n193.0\n\n\n5\n33.1\n178.0\n\n\n\n\n\n\n\nAnd then find the best qualitative feature:\n\nqual_selector = SelectKBest(f_classif, k=6)\nX_new = qual_selector.fit_transform(X_train, y_train)\n# scikit-learn does not keep names ):\n# https://stackoverflow.com/a/41041230\nX_train.loc[:, qual_selector.get_support()].head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 13 C (o/oo)\nIsland_Biscoe\n\n\n\n\n1\n45.1\n14.5\n215.0\n5000.0\n-25.46569\n1\n\n\n2\n41.4\n18.5\n202.0\n3875.0\n-25.42621\n0\n\n\n3\n39.0\n18.7\n185.0\n3650.0\n-26.03442\n0\n\n\n4\n50.6\n19.4\n193.0\n3800.0\n-24.97134\n0\n\n\n5\n33.1\n16.1\n178.0\n2900.0\n-26.15775\n0\n\n\n\n\n\n\n\nThe result seems to be on which island does the penguin reside.\nWe include all three Island features since only checking whether they live on Biscoe does not make sense.\n\ncols = ['Culmen Length (mm)', 'Flipper Length (mm)',\n        'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\nPerformance on Logistic Regression\nWe can also recursively consider smaller sets of features, and evaluate how this performs on a certain model.\nThe latter, with cross validation, is availiable as a built-in function RFECV.\nFor the cross-validation step, we use a strafied 5-fold cross-validator on logistic regression.\n\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\n\nrfecv = RFECV(\n    estimator=LogisticRegression(),\n    step=1,\n    cv=KFold(10),\n    scoring=\"accuracy\",\n    min_features_to_select=2\n)\nrfecv.fit(X_train, y_train)\nX_train.loc[:, rfecv.get_support()].head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Dream\nSex_FEMALE\n\n\n\n\n1\n45.1\n14.5\n7.63220\n-25.46569\n0\n1\n\n\n2\n41.4\n18.5\n9.59462\n-25.42621\n0\n0\n\n\n3\n39.0\n18.7\n9.22033\n-26.03442\n1\n0\n\n\n4\n50.6\n19.4\n9.28153\n-24.97134\n1\n0\n\n\n5\n33.1\n16.1\n9.04218\n-26.15775\n1\n1\n\n\n\n\n\n\n\nRFECV ranks 6 features as equally important - but why?\nHere are the importance scores that RFECV determined:\n\nX_train.columns\nrfecv.ranking_\nd = pd.DataFrame(columns=X_train.columns)\n# Use `df.loc[len(df)] = arr` – rafaelc Oct 8, 2019 at 19:43\n# https://stackoverflow.com/q/58292901\nd.loc[\"Importance\"] = rfecv.ranking_\nd\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\nImportance\n1\n1\n5\n9\n1\n1\n2\n1\n4\n8\n6\n7\n1\n3\n\n\n\n\n\n\n\nAnd here is the mean test accuracy of the process with different number of features selected:\n\nn_scores = len(rfecv.cv_results_[\"mean_test_score\"])\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Mean test accuracy\")\nplt.errorbar(\n    range(2, n_scores + 2),\n    rfecv.cv_results_[\"mean_test_score\"],\n    yerr=rfecv.cv_results_[\"std_test_score\"],\n)\nplt.title(\"Recursive Feature Elimination with correlated features\")\nplt.show()\n\n\n\n\nWe see that, with 10-fold cross-validation, the mean test accuracy only got to 100% after selecting 6 features.\nTherefore, RFECV is marking them as “equally important”.\nWe are thus resorting to the columns we selected in Univariate statistical tests.\n\n\n\nModeling\n\nLogistic regression\nThe modeling process is more straightforward. Here, we use logistic regression:\n\nfrom sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\nLR.fit(X_train[cols], y_train)\nLR.score(X_train[cols], y_train)\n\n0.96875\n\n\nThe model performed well, although it did not reach 100% accuracy on the training set.\n\n\nRandom forest\nWhen we maintaining an ensemble of decision trees, we can let them vote on the best category.\nThis method was state-of-the-art before the rise of neural networks. 1\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'n_estimators': [5, 10, 15, 20, 25, 30],\n    'max_depth': [2, 5, 7, 9, 13]\n}\n\nRF = RandomForestClassifier(max_features=1)\n\nGRF = GridSearchCV(RF, param_grid, cv=10)\nGRF.fit(X_train[cols], y_train)\nGRF.best_params_\n\n{'max_depth': 7, 'n_estimators': 15}\n\n\nHere, we use a grid search with cross-validation (GridSearchCV) to identify the best hyperparameters among a predefined param_grid.\nOur GridSearchCV has cross-validated our Random Forest models, and selected the best parameters.\nWe can then access our best estimator with GRF.best_estimator_."
  },
  {
    "objectID": "posts/penguins/index.html#testing",
    "href": "posts/penguins/index.html#testing",
    "title": "Penguins",
    "section": "Testing",
    "text": "Testing\nNow that we have two models, we can test their performance on the test set.\n\nLogistic regression\nFirst, gather the data and run logistic regression on it:\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\",\n                 \"Date Egg\", \"Comments\", \"Region\"], axis=1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis=1)\n    df = pd.get_dummies(df)\n    return df, y\n\n\nX_train, y_train = prepare_data(train)\n\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n1\n45.1\n14.5\n215.0\n5000.0\n7.63220\n-25.46569\n1\n0\n0\n1\n0\n1\n1\n0\n\n\n2\n41.4\n18.5\n202.0\n3875.0\n9.59462\n-25.42621\n0\n0\n1\n1\n0\n1\n0\n1\n\n\n3\n39.0\n18.7\n185.0\n3650.0\n9.22033\n-26.03442\n0\n1\n0\n1\n0\n1\n0\n1\n\n\n4\n50.6\n19.4\n193.0\n3800.0\n9.28153\n-24.97134\n0\n1\n0\n1\n1\n0\n0\n1\n\n\n5\n33.1\n16.1\n178.0\n2900.0\n9.04218\n-26.15775\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n\n\n\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[cols], y_test)\n\n0.9411764705882353\n\n\nThis score is less than we had in the training set, and also not what this blog post expected (100%).\nWe can take a look at what happened through plotting the decision regions (code source):\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\n\ndef plot_regions(model, X, y):\n\n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n\n    fig, axarr = plt.subplots(1, len(qual_features), figsize=(7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n\n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n        XY = pd.DataFrame({\n            X.columns[0]: XX,\n            X.columns[1]: YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n\n        # use contour plot to visualize the predictions\n        axarr[i].contourf(xx, yy, p, cmap=\"jet\", alpha=0.2, vmin=0, vmax=2)\n\n        ix = X[qual_features[i]] == 1\n        # plot the data\n        axarr[i].scatter(x0[ix], x1[ix], c=y[ix], cmap=\"jet\", vmin=0, vmax=2)\n\n        axarr[i].set(\n            xlabel=X.columns[0],\n            ylabel=X.columns[1]\n        )\n\n        patches = []\n        for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n            patches.append(Patch(color=color, label=spec))\n\n        plt.legend(title=\"Species\", handles=patches, loc=\"best\")\n\n        plt.tight_layout()\n\n\nplot_regions(LR, X_test[cols], y_test)\n\n\n\n\nOn each island, it looked like the logistic regression found approximate lines that separated each species.\nHowever, there is some overlap between each region, so accuracy was less than 100%.\n\n\nRandom forest\nHow did random forest do? Let us find out.\n\nGRF.best_estimator_.score(X_test[cols], y_test)\n\n0.9705882352941176\n\n\nThis is marginally better, but still not quite at 100% yet.\nAgain, here are the decision boundaries:\n\nplot_regions(GRF.best_estimator_, X_test[cols], y_test)\n\n\n\n\nHere, the shape of the decision boundaries suggest overfitting - perhaps we should revisit param_grid and find better hyperparameters."
  },
  {
    "objectID": "posts/penguins/index.html#footnotes",
    "href": "posts/penguins/index.html#footnotes",
    "title": "Penguins",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGomes, H.M., Bifet, A., Read, J. et al. Adaptive random forests for evolving data stream classification. Mach Learn 106, 1469–1495 (2017). https://doi.org/10.1007/s10994-017-5642-8↩︎"
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Perceptron",
    "section": "",
    "text": "Image credits: BruceBlaus under CC BY 3.0 via Wikimedia Commons.\nPython source: 451-blog/perceptron.py at main · doabell/451-blog\nInstructions can be found at Implementing the Perceptron Algorithm."
  },
  {
    "objectID": "posts/perceptron/index.html#experiments",
    "href": "posts/perceptron/index.html#experiments",
    "title": "Perceptron",
    "section": "Experiments",
    "text": "Experiments\n\nLinearly separable data\nThrough trial and error, we generate some linearly separable data and test the perceptron algorithm on it.\n\n# Data\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom matplotlib import font_manager\nfont_manager.fontManager.addfont(\"C:\\Windows\\Fonts\\FiraSans-Regular.ttf\")\nplt.rcParams[\"font.family\"] = \"Fira Sans\"\nnp.random.seed(283)\n\nn = 120\np_features = 3\n\nX, y = make_blobs(\n    n_samples=n,\n    n_features=p_features - 1,\n    centers=[(-1.7, -1.7), (1.7, 1.7)]\n)\n\nfig = plt.scatter(X[:, 0], X[:, 1], c=y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThere is a clear line that separates the two blobs here.\nLet us find out if our perceptron algorithm can find that line.\nFirst, we fit the algorithm with at most 1000 steps:\n\n# Algorithm\nfrom perceptron import Perceptron\n\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\nThen, we can visualize the accuracy of each iteration:\n\n# score history\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nWe can see that for this example, w only changed 4 times.\nThen the model reached 100% accuracy and stopped running.\nWe can then visualize the dividing line found by the perceptron:\n\n# visualize\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color=\"black\")\n\n\nfig = plt.scatter(X[:, 0], X[:, 1], c=y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n# score, should be 1\np.score(X, y)\n\n1.0\n\n\nThat line divided the two blobs right in the middle, resulting in a 1.0 accuracy. Not bad!\n\n\nNot linearly separable\nNext, we consider 2-d data that is not linearly separable.\n\nnp.random.seed(227)\n\nn = 120\np_features = 3\n\nX, y = make_blobs(\n    n_samples=n,\n    n_features=p_features - 1,\n    centers=[(-1.7, -1.7), (1.7, 1.7)]\n)\n\nfig = plt.scatter(X[:, 0], X[:, 1], c=y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThe dividing line should be at about the same position, but now the two blobs overlap with each other.\nLet us observe whether the perceptron algorithm still manages to find that line.\n\n# Algorithm\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\n# score history\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nWe can see that given data that is not linearly separable, w does not converge.\nInstead, the algorithm iterates the full 1000 iterations, and accuracy fluctuates between around 0.95 and 0.97.\nThis is expected, because there is no line that perfectly separates these two blobs.\nAs for the dividing line:\n\n# visualize\ndef draw_line(w, x_min, x_max):\n    x = np.linspace(x_min, x_max, 101)\n    y = -(w[0]*x + w[2])/w[1]\n    plt.plot(x, y, color=\"black\")\n\n\nfig = plt.scatter(X[:, 0], X[:, 1], c=y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n# score, should not be 1\np.score(X, y)\n\n0.95\n\n\nAt the cost of ~6 misclassified data points, the algorithm yielded an accuracy of 0.95. Good enough.\n\n\nHigher dimensions\nFinally, we increase the number of features to reach higher dimensions.\nVisualization will be vard beyond 3-d, so we will instead look at the score history during training only.\nWe will be using 8 features (including y) and 1500 data points.\n\nnp.random.seed(149)\n\nn = 1500\np_features = 7\n\n# make centers with correct dimensions\nc1 = np.empty(p_features - 1)\nc1.fill(-1.5)\n\nc2 = np.empty(p_features - 1)\nc2.fill(1.5)\n\nX, y = make_blobs(\n    n_samples=n,\n    n_features=p_features - 1,\n    centers=[c1, c2]\n)\n\n# Algorithm\np = Perceptron()\np.fit(X, y, max_steps=1000)\n\n# score history\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\n# score\np.score(X, y)\n\n1.0\n\n\nOn this dataset, our perceptron algorithm reached 100% accuracy!\nAs for the accuracy during model fitting, the process took more than 500 iterations, but ended up achieving a perfect score at the end.\nBecause our perceptron converged, we can say that this particular dataset is linearly separable."
  },
  {
    "objectID": "posts/perceptron/index.html#note-on-complexity",
    "href": "posts/perceptron/index.html#note-on-complexity",
    "title": "Perceptron",
    "section": "Note on complexity",
    "text": "Note on complexity\n\nWhat is the runtime complexity of a single iteration of the perceptron algorithm update as described by Equation 1? Assume that the relevant operations are addition and multiplication. Does the runtime complexity depend on the number of data points n? What about the number of features p?\n\nThe relevant line is Line 51 in perceptron.py:\nself.w = self.w + ((y_[i] * self.w @ X_[i]) &lt; 0) * y_[i] * X_[i]\nWe can use numpy.ndarray.shape to find out the shape of these variables involved.\n\n# From line 39\nX_ = np.append(X, np.ones((X.shape[0], 1)), 1)\n\nprint(f\"Shape of self.w is {p.w.shape}\")\nprint(f\"Shape of y_[i] is {y[0].shape}\")\nprint(f\"Shape of X_[i] is {X_[0].shape}\")\n\nShape of self.w is (7,)\nShape of y_[i] is ()\nShape of X_[i] is (7,)\n\n\nWe can see that self.w and X_[i] have length p, while y[i] is a single integer.\nTherefore, the runtime complexity is O(p) and does not depend on the number of data points n.\nBy intuition, we do not have to look at all the data points when updating self.w. We only need to do that when determining the score, but that is elsewhere in the function and not described by Equation 1."
  },
  {
    "objectID": "posts/project/index.html",
    "href": "posts/project/index.html",
    "title": "Housing Price Prediction",
    "section": "",
    "text": "Our project utilized (classical and deep) machine learning techniques to analyze and predict housing prices across different counties in the United States, with a focus on understanding the most impactful factors. We were able to carefully identify, select, and merge datasets containing variables related to housing, population, and geo-spatial features. After constructing several models, including classical ones via scikit-learn and a neural network via PyTorch, we found that the highest accuracy was achieved by our Random Forest model, followed by Gradient Boosting regression; the neural network performed relative poorly. These models helped us to identify some of the most important factors of housing prices, including median income, education attainment, and perhaps surprisingly, longtitude. The relevant source code is hosted on Github."
  },
  {
    "objectID": "posts/project/index.html#abstract",
    "href": "posts/project/index.html#abstract",
    "title": "Housing Price Prediction",
    "section": "",
    "text": "Our project utilized (classical and deep) machine learning techniques to analyze and predict housing prices across different counties in the United States, with a focus on understanding the most impactful factors. We were able to carefully identify, select, and merge datasets containing variables related to housing, population, and geo-spatial features. After constructing several models, including classical ones via scikit-learn and a neural network via PyTorch, we found that the highest accuracy was achieved by our Random Forest model, followed by Gradient Boosting regression; the neural network performed relative poorly. These models helped us to identify some of the most important factors of housing prices, including median income, education attainment, and perhaps surprisingly, longtitude. The relevant source code is hosted on Github."
  },
  {
    "objectID": "posts/project/index.html#introduction",
    "href": "posts/project/index.html#introduction",
    "title": "Housing Price Prediction",
    "section": "Introduction",
    "text": "Introduction\nHousing prices are a crucial aspect of the American economy. It directly influences various sectors, including the real estate industry, financial markets, and consumers’ purchasing power. Understanding the determinants of housing prices is therefore essential for potential house buyers, sellers, renters, and city planners alike.\nThus, this project aims to dissect the relationship between housing prices, demographic, and socioeconomic indicators across US counties. We believe our project can inform these relevant sectors, empower stakeholders, and pave the way for future work on what actually influences housing prices.\nHo, Tang, and Wong (2021) studys property price prediction using three machine learning algorithms, including Support Vector Machines (SVM), Random Forest (RF), and a Gradient Boosting Machine (GBM). Their study finds that RF and GBM provided superior performance over SVM. In our work, we also incorporate Random Forest models due to their demonstrated efficacy and robustness. Interpretability is also a key advantage of Random Forest models.\nSimilarly, Thamarai and Malarvizhi (2020) highlights the importance of utilizing different house-level attributes for price prediction, such as the number of bedrooms, age of the house, and proximity to essential facilities like schools and shopping malls. We too consider a multitude of housing factors, albeit not directly due to the nature of the US Census, and additional socioeconomic features in our analysis.\nIn the meantime, Zulkifley et al. (2020) emphasizes the importance of data mining in predicting housing prices, and finds that locational and structural attributes significantly influence such predictions. As such, our comprehensive set of features include geospatial information, including position and land area, in line with this recommendation.\nThrough our analysis, we aim to contribute to this growing body of research, and provide further, timely insight into housing price prediction and its influencing factors."
  },
  {
    "objectID": "posts/project/index.html#value-statement",
    "href": "posts/project/index.html#value-statement",
    "title": "Housing Price Prediction",
    "section": "Value Statement",
    "text": "Value Statement\nThese are the users and potential beneficiaries of our project:\n\nPotential home-buyers and renters, who can make informed decisions when purchasing properties\nReal estate professionals, who can provide better guidance to their clients\nUrban planners and policymakers, who can make more informed decisions regarding zoning, land use, and housing policies\nSocial workers, who can identify driving factors of inequality in housing and rent prices, and prioritize advocating for change in these key areas\n\nThese are the potentially excluded from benefit or harmed by our project:\n\nMarginalized populations, who may be underrepresented or misrepresented in the data, so our model may not accurately reflect their needs and wants\nResidents of certain reguins with inaccurate predictions, leading to incorrect conclusions about said region, and / or suboptimal buying / renting decisions\n\nMy personal reasons for working on this problem:\nAfter the most recent Fed rate hike, for the first time, since 2007, the US Federal Funds Target Range has been pushed above 5%. This impacts the whole economy, as well as the housing market, including fixed-rate mortgages, so I thought it interesting to look at housing prices across the country.\nWill our project make the world a more equitable, just, joyful, peaceful, or sustainable place?\nUnder the assumption that our data sources represent different demographics and regions fairly, yes, we are confident that with the additional transparency and insight provided by our project, the US housing market has the potential to become more equitable, just, and sustainable."
  },
  {
    "objectID": "posts/project/index.html#materials-and-methods",
    "href": "posts/project/index.html#materials-and-methods",
    "title": "Housing Price Prediction",
    "section": "Materials and Methods",
    "text": "Materials and Methods\n\nData\nWe utilized two main data sources: Zillow (2011), and the U.S. Census Bureau (2021). We also collected other explanatory predictors, such as land area and geospatial information (longitude & latitude), that are also from the Census Bureau.\nThe US census data is from the 2017-2021 ACS (American Community Survey) 5-Year PUMS (Public Use Microdata Sample), accessed through the Census API per PUMS Documentation. This dataset contains information collected by the Census Bureau about individual people or housing units. We believe this is a representative and authoritative source of information for various demographic and economic factors, that can influence housing prices and affordability.\nZillow Housing Data is a collection of datasets from Zillow, a real-estate marketplace company. It contains features include housing prices, rental prices, city, state, etc. The relevant data can be accessed here, under “ZHVI All Homes (SFR, Condo/Co-op) Time Series, Smoothed, Seasonally Adjusted($), by County”.\nLastly, we collected geospatial data for each county from US Census Bureau (2022): U.S. Gazetteer Files 2022.\nIn our finalized dataset, each row corresponds to a US county as an observation. Each observation contains the average home value, as well as other explanatory factors like median age, income, and education attainment.\nDespite our best efforts in collecting comprehensive data, it’s important to acknowledge some limitations. While we were able to gather county-level data from sources like Zillow and the US Census, certain house-level factors, like the exact condition of a house, or neighborhood characteristics, might not be fully represented. Moreover, while the US Census publishes widely-used datasets of high quality, it may still underrepresent certain marginalized communities, potentially introducing biases into our findings.\n\n\nSelected Variables\nA full list of these variable names can be found here, under “2021 ACS Detailed Tables Variables”.\nWe were able to select the following independent variables from the ACS dataset:\n\nB01002_001E: total median age\nB01003_001E: total population\nB08134_001E: travel time to work\nB15012_001E: number of Bachelor’s degrees\nB19013_001E: median household income, inflation-adjusted\nB19083_001E: gini index of income inequality\nB23025_005E: civilian labor force, unemployed\nB25001_001E: total housing units\nB25002_002E: occupancy is “occupied”\nB25018_001E: median number of rooms\nB25035_001E: median year of construction\nB25040_002E: houses with heating fuel\nB25064_001E: median gross rent\nB25081_001E: count of housing units with a mortgage\n\nAdditional independent variables include coordinates for each county, and land and water area (as a percentage of total area).\nOur dependent variable is Zillow’s Home Value Index for 2021, which reflects the “typical home value” for houses in a county. Zillow reports this index per month, so the average is taken for the entirety of 2021.\nFor future bias auditing, we also kept these relevant variables:\n\nB02001_002E: White alone\nB02001_003E: Black or African American alone\nB02001_004E: American Indian and Alaska Native alone\nB02001_005E: Asian alone\nB02001_006E: Native Hawaiian and Other Pacific Islander alone\nB02001_007E: Some other race alone\nB02001_008E: Two or more races\nB02001_009E: Two races including Some other race\nB02001_010E: Two races excluding Some other race, and three or more races\n\n\n\nVariable importance\nAnother feature we paid attention to was variable importance. How did each variable contrinbute to each model’s prediction of housing prices?\nTo ensure consistency between models with different levels of interpretability, we chose permutation importance as our measurement of feature importance. Essentially, by permutating a certain feature column, we “mix up” that column’s influence on the model predictions, so that the resulting difference in performance can be attributed to that feature column.\nHowever, if multicollinearity or correlation between features are present, then permutating one column has little effect on model performance, because the information provided by that feature can be accessed in another, correlating feature. Therefore, it is also important to plot and inspect a dendrogram for our selected features, to identify any correlations."
  },
  {
    "objectID": "posts/project/index.html#results",
    "href": "posts/project/index.html#results",
    "title": "Housing Price Prediction",
    "section": "Results",
    "text": "Results\nWe fitted our selected variables against several popular classical models from the scikit-learn package.\nThese include Linear regression, Linear regression with SGD, Ridge regression, Gradient Boosting regression, Support Vector Machines, and Random Forest models.\nWe also proceeded to build our own fully-connected neural network using PyTorch.\n\nModel Performance\nIn order to evaluate those algorithms we used, we implemented two metrics to measure the relative performance:\n\nCoefficient of determination. This is the R^2 score, or the residual sum of squares divided by the total sum of squares, for a certain model. The best possible score is 1.0, and because the model can be arbitrarily worse, scores can be negative, and there is no worst score. When the prediction residuals have zero mean, this corresponds to the percent of variance explained by the model.\nMean Squared Error (MSE). This represents the average of the squared differences between the predicted and actual values. In other words, it quantifies the difference between the estimator (the model’s prediction) and the actual value.\n\nThe following table lists the R^2 scores and MSEs for our models, in order of decreasing performance:\n\nModel Performance on Test Set\n\n\n\nScore\nMSE (\\times 10^9)\n\n\n\n\nRandom Forest\n0.9109\n1.6078\n\n\nGradient Boosting\n0.8612\n2.5038\n\n\nLinear Regression with SGD\n0.7677\n4.1899\n\n\nRidge Regression\n0.7675\n4.1939\n\n\nLinear Regression\n0.7673\n4.1976\n\n\nSupport Vector Machine\n0.7327\n4.8206\n\n\nNeural Network\n0.5231\n8.6008\n\n\n\nWe observe that tree-based models, like Random Forest and Gradient Boosting (an improvement on Decision Trees), perform well on our dataset, followed by linear regression-like models, and then Support Vector Machines.\nThe fully-connected neural network performed poorly on our regression task.\n\n\nFeature Importance\n\n\n\nPermutation Feature Importance for our Random Forest model\n\n\nHere are the top 5 features ordered by importance, as determined by our Random Forest model:\n\nB19013_001E: median household income, inflation-adjusted\nINTPTLONG: longitude (decimal degrees)\nB15012_001E: number of Bachelor’s degrees\nB25035_001E: median year of construction\nB19083_001E: gini index of income inequality\n\nNote that feature importance is not the same as a coefficient in linear regression, as the relationship may not be linear. In our Random Forest regressor model, particularly, the corresponding explanation is most likely segmented and discrete.\n\n\nFeature correlation\n\n\n\nDendrogram and Heatmap for our selected features\n\n\nThe above figure shows the dendrogram (evaluated by Ward’s linkage) and heatmap of correlation between each independent variable in our model.\nOn the left, we can observe that B01003_001E (total population) is correlated with B08134_001E (travel time to work) and B25001_001E (total housing units). This is reasonable because the number of housing units, and subsequently travel time to work, does depend on how many people are inside the county.\nOn the right, we can see that land and water area are strongly correlated, as shown by two black squares. This is due to land and water area being percentages instead of absolute values, so they will always add up to one."
  },
  {
    "objectID": "posts/project/index.html#concluding-discussion",
    "href": "posts/project/index.html#concluding-discussion",
    "title": "Housing Price Prediction",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nOur project worked in the following ways:\nWe were able to apply our learning in the course to predict housing prices across US counties. We also learned a lot more about machine learning models, in particular the nuances and technicalities concerning feature importance, correlated features, and\nWe also applied our learning to implement a fully-connected neural network, and verified its subpar performance on tabular data.\nOur project goals were partially met, in that our data processing, model implementation, model evaluation, and analysis on feature importance are complete.\nWe were hoping to identify a comprehensive framework for auditing bias, but were unable to find a methodology that goes beyond arbitrarily comparing cross-sectional trends and biases. Therefore, we did not deliver any bias audits, despite having relevant data available.\nGiven more time, we may be able to focus on bias auditing, and reproducing models from previous literature to evaluate our model’s performance. There is also an opportunity for field research on our model’s predictions, potentially through interviews, simulated auctioning events, or case studies for existing homes."
  },
  {
    "objectID": "posts/project/index.html#group-contributions-statement",
    "href": "posts/project/index.html#group-contributions-statement",
    "title": "Housing Price Prediction",
    "section": "Group Contributions Statement",
    "text": "Group Contributions Statement\nWe met and worked together on this project every week.\nBell was mainly responsible for data processing, visualization, and fitting classical models. He wrote relevant code to clean and merge the data. He also generated insightful visualizations such as an interactive map. Additionally, he designed and executed numerous experiments, with a particular focus on model validation and performance assessment. Then he carried out several classical model experiments, and in particular related them to feature importance.\nJiayi primarily focused on training a neural network from scratch. He coded the data loader, training loops, and the fitting of the PyTorch model. He also performed several experiments related to hyperparameter tuning. He helped in finding suitable datasets and selecting relevant variables, as well as identifying and plotting multicollinear features. Additionally, he gathered materials into a preliminary version of this blog post.\nThe project proposal, presentation slides, and this blog post are of equal contribution between the two authors. (This excludes the “personal reasons” and “Personal Reflection” sections.)"
  },
  {
    "objectID": "posts/project/index.html#personal-reflection",
    "href": "posts/project/index.html#personal-reflection",
    "title": "Housing Price Prediction",
    "section": "Personal Reflection",
    "text": "Personal Reflection\n\nWhat did you learn from the process of researching, implementing, and communicating about your project?\n\nA lot of data science is data cleaning and tuning hyperparameters, and this was heartfelt throughout this project. Even though we stayed in the realm of organized and clean datasets with clear cross-walks (FIPS codes), the data preparation still took a significant amount of our time.\nThrough comparing models in scikit-learn and our own neural network in PyTorch, I can now appreciate the differences, similarities, and nuances for each model better than before.\nOne particularly valuable learning outcome was using PyTorch to build a data loader and train a neural network.\n\nHow do you feel about what you achieved? Did meet your initial goals? Did you exceed them or fall short? In what ways?\n\nI am happy with the progression of our project, from an empty folder to multiple notebooks with data, models, visualizations, and even a map.\nMy initial goals concerning writing quality code, drafting and revising this blog post, and collaborating on the project presentation were achieved.\nI exceeded these goals in incorporating additional, careful analysis of our independent variables.\nThis blog post was submitted after the “best by” date, due to collaborator availability constraints.\n\nIn what ways will you carry the experience of working on this project into your next courses, career stages, or personal life?\n\nTwo years ago, I submitted a blog post on Jails, Voter ID Laws, and Turnout for my Econ Stats course. A lot has changed to both the world and myself in these two years, but one recurring theme is the intersection between data, models, and real-life impact, whether it be measuring, understanding, or changing the world around us.\nThis project has been the continuation of that theme, and I am confident that I will continue to understand the world through data, be mindful of the effect of such understanding, and show reverence to the struggles and triumphs of every individual, represented behind rows in a dataframe."
  },
  {
    "objectID": "posts/unsupervised/index.html",
    "href": "posts/unsupervised/index.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nfrom matplotlib import font_manager\nfont_manager.fontManager.addfont(\"C:\\Windows\\Fonts\\FiraSans-Regular.ttf\")\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nnp.random.seed(42)\n\nsns.set_theme()\nsns.set(font=\"Fira Sans\")\nimport svd\nPython source: 451-blog/svd.py at main · doabell/451-blog\nImage credits: atlas.brussels\nInstructions can be found at Unsupervised Learning with Linear Algebra."
  },
  {
    "objectID": "posts/unsupervised/index.html#svd-image-compression",
    "href": "posts/unsupervised/index.html#svd-image-compression",
    "title": "Unsupervised Learning",
    "section": "SVD Image Compression",
    "text": "SVD Image Compression\nIn part 1, we explore image compression with singular value decomposition (SVD).\nSVD is a matrix decomposition (or factorization) method, and is used in principal component analysis (PCA) - see discussions on the relationship between SVD and PCA, on Cross Validated and the Math Stack Exchange.\n\nReconstruction\nHere is a brief demo from the blog post instructions.\nWe first generate a vector representing a greyscale image:\n\na_1 = np.random.randint(1, 3, (5, 3))\na_2 = np.random.randint(1, 3, (3, 7))\n\nA = a_1 @ a_2 + 0.1*np.random.randn(5, 7)\n\nIn numpy, the actual SVD process is a one-liner.\nsigma contains the singular values of A; we reconstruct A after creating D, the corresponding diagonal matrix.\n\nU, sigma, V = np.linalg.svd(A)\n\nD = np.zeros_like(A,dtype=float)\nD[:min(A.shape),:min(A.shape)] = np.diag(sigma)\n\nA_recon = U @ D @ V\n\nsvd.compare_images(A, A_recon)\n\n\n\n\nWe can visually confirm, at a glance, that these two images are indeed identical.\n\n\nCompression\nBy keeping only the first k columns of U, k values of D, and k rows in V, we can reconstruct something similar to the same image, with less storage.\n\nk = 2\nU_ = U[:,:k]\nD_ = D[:k, :k]\nV_ = V[:k, :]\n\nA_ = U_ @ D_ @ V_\n\nsvd.compare_images(A, A_)\n\n\n\n\nHere, we have constructed a similar image, with less storage (by shrinking U, D, and V).\n\n\nReal-life image\nImage by Wells Baum on Unsplash.\n\nurl = \"https://unsplash.com/photos/L-n-SIvQBHw/download?force=true\"\n# url = \"https://unsplash.com/photos/L-n-SIvQBHw/download?force=true&w=640\"\n\nxlim = (0, 340)\nylim = (1390, 1060)\n\nmta = svd.url2img(url)\n\nsvd.zoom_images(mta, xlim, ylim)\n\n\n\n\n\nNumber of components\nWe first try compressing by specifying the number of components.\nSince this is a large image, 20 might be a good guess.\n\nmta_ = svd.svd_reconstruct(mta, k=20)\n\nsvd.zoom_images(mta_, xlim, ylim, orig=mta)\n\n\n\n\nAt k=20, we can make out the “Subway” text, but not the smaller text on the left.\nThe contrast is also lower; the reconstructed dark parts are not as dark.\nWe experiment with various values of k:\n\nsvd.svd_experiment(mta)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe observe the zoomed-in text getting clearer as we increase k.\nThe contrast also approaches the original image.\n\n\nDiscussion on size\nWe investigate the compression rate of image compression using SVD, which is the size needed for reconstruction as a fraction of the size of the original image.\nUnlike the flexible (memory) size of Python’s int, each item in NumPy’s arrays have a fixed size of numpy.ndarray.itemsize, so the total size of a NumPy array, numpy.ndarray.nbytes, is simply numpy.ndarray.itemsize * numpy.ndarray.size.\n(No, the sparse matrices are in SciPy.)\nNumPy’s numpy.single is “compatible with C float”1, so this is typically 8 bytes. (You can fire up gcc to make sure.) Even if it’s not, the size would stay constant, so as a proportion this can be ignored.\nOne other thing to ignore is NumPy arrays’ overhead, which contributes to the difference observed between numpy.ndarray.nbytes and sys.getsizeof(). As our NumPy array gets larger, however, this difference becomes very small, so we will ignore this too.\nFor the SVD of a matrix image of size m \\times n, the size of U is m \\times m, and the size of V is n \\times n. While D has shape \\min(m, n) \\times \\min(m, n), it is a diagonal matrix that can be reproduced from a 1-D vector of size \\min(m, n).\nWhen compressed by keeping the top k singular values, the sizes of U, V, and D becomes m \\times k, n \\times k, and k \\times k (constructable with k) respectively.\nSince the original image has size m \\times n, the resulting compression factor would be k\\frac{m+n+1}{mn}.\n\n\nCompression factor\nAt k=200, the resulting compression factor is 14.30%, and the image preserves much of its original details.\nWe can observe how a larger compression factor performs:\n\nmta_ = svd.svd_reconstruct(mta, cf=0.2)\n\nsvd.zoom_images(mta_, xlim, ylim, orig=mta, text=f\"20% compression\")\n\n\n\n\nEven in the zoomed-in portion, apart from some spots, the reconstructed image is very similar to the original image.\n\n\nEpsilon\nWe can also experiment with epsilon for the singular values:\n\nmta_, U, sigma, V = svd.svd_reconstruct(mta, epsilon=2000, store_vals=True)\n\nm, n = mta.shape\n\nk = sigma.shape[0]\n\npercent = 100 * k * (m + n + 1) / (m * n)\n\nsvd.zoom_images(mta_, xlim, ylim, orig=mta, text=f\"epsilon = 2000\\n{percent:.2f}% compression\")\n\nprint(f\"k = {k}\")\n\nk = 143\n\n\n\n\n\nAn epsilon = 2000 for this image translates to k=143 at around 10.22% compression, similar to 10.72% at k=150 above."
  },
  {
    "objectID": "posts/unsupervised/index.html#what-about-jpeg",
    "href": "posts/unsupervised/index.html#what-about-jpeg",
    "title": "Unsupervised Learning",
    "section": "What about JPEG?",
    "text": "What about JPEG?\nJPEG image compression does not use SVD, but instead uses DCT (discrete cosine transform).\nFrom Wikipedia’s page on JPEG:\n\nThe human eye is good at seeing small differences in brightness over a relatively large area, but not so good at distinguishing the exact strength of a high frequency brightness variation.\n\nTherefore, a pre-determined quantization matrix can take advantage of such traits and yield a higher compression ratio.\nSVD compression, on the other hand, does not seem to utilize human perception.\nIt is also notable that DCT is present in many well-known formats, including HEIF, MPEG, H.264, MP3, AAC, and Opus.\nTherefore, there might be system- or hardware-level optimizations possible with DCT-compressed file formats over alternative methods like SVD."
  },
  {
    "objectID": "posts/unsupervised/index.html#footnotes",
    "href": "posts/unsupervised/index.html#footnotes",
    "title": "Unsupervised Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.single↩︎"
  }
]